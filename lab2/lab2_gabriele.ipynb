{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this assignment will be **automatically graded**. Please take note of the following:\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- You can add additional cells, but it is not recommended to (re)move cells. Cells required for autograding cannot be moved and cells containing tests cannot be edited.\n",
    "- You are allowed to use a service such as [Google Colaboratory](https://colab.research.google.com/) to work together. However, you **cannot** hand in the notebook that was hosted on Google Colaboratory, but you need to copy your answers into the original notebook and verify that it runs succesfully offline. This is because Google Colaboratory destroys the metadata required for grading.\n",
    "- Name your notebook **exactly** `{TA_name}_{student1_id}_{student2_id}_lab{i}.ipynb`, for example `wouter_12345_67890_lab1.ipynb` (or elise or stephan, depending on your TA), **otherwise your submission will be skipped by our regex and you will get 0 points** (but no penalty as we cannot parse your student ids ;)).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = \"Davide Belli, Gabriele Cesa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "931b3dfcc3a02b92b499929fb27299cb",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e83ecfc2751cf2e6ff05d0c01d311673",
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Deep Q-Network (DQN) (10 (+ 2 bonus) points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e27fe8f72a248bbcf1f7a21e5550e657",
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11a9c014ee5fbe790ce999428cc22658",
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9692b7acb09d018d9f80ce95685b81d5",
     "grade": false,
     "grade_id": "cell-bf2ac21267daffbb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you think of a way in which we can still use a tabular approach? Why would this work and can you think of an example problem where this would not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ffce6fca4071a1b543186db1b74cc98",
     "grade": true,
     "grade_id": "cell-b0fa2cb0c2cd2a63",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# Check Me!\n",
    "\n",
    "We can use binning (aggregation) to approximate the infinite number of possible state with a discrete number of state representing interval over the feature's domains. \n",
    "\n",
    "This approach would not work for problems in which the optimal (or sufficient) action for every state is unique and can be expressed as a function of the state feature values. In this case an approximation of the infinite states using binning approach would result in a wrong action for that state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd66b44d93f348df1e0ef8353377c879",
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84b9c38718c952ef8e62486fc9bf5e4a",
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4ef7d14363dc2aa4beb638856c57a58c",
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b9a48f9aee9ebc46da01c6f11cd789a",
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7fc82889691dbd60ff9469b770744fcc",
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b3265bef151a12fe6969c378af76be2",
     "grade": false,
     "grade_id": "cell-b5b012e42dd2029e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What could be a problem with doing gradient updates on a sequence of state, action pairs $((s_t, a_t), (s_{t+1}, a_{t+1}) ...)$ observed while interacting with the environment? How will using *experience replay* help to overcome this (potential problem)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75e1a8b00b2bfa9b7dd8805b371c6a4e",
     "grade": true,
     "grade_id": "cell-70a2e59541668a25",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# Check me\n",
    "\n",
    "The problem is that successive updates in the original Q-learning approach are corralated to each other. This increases the variance in the learning and can cause instabilities. \n",
    "Experience replay can be used to solve this problem by saving an history of experiences from interacting with the environment and then learning a model by sampling experiences from this history, in order to avoid sequential updates (from Sutton and Barto, pp. 440-441)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9b3bbd8aaf3aade515736d0d07917a61",
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the `push` function that adds a transition to the replay buffer, and the sample function that returns a batch of samples. It should keep at most the maximum number of transitions. Also implement the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93a9f55f3950fe63b44aa84c5fd7f793",
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self) < self.capacity:\n",
    "            self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6865749b3a8810bdaaf1604a9cea42e7",
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.04073589, -0.01653962, -0.02860587,  0.00222345]), 0, 1.0, array([ 0.0404051 , -0.2112399 , -0.0285614 ,  0.28574539]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c742d499c0f9b7f10d1c0c3a085236a",
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61d26d0dec0133f2aa737ed4711d6e08",
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "270ab31d4bb29dc9a05223c16a4967a7",
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    return .05 if it > 1000 else (1- it * 0.95 /1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1a81dd07e1b7a98d2cd06ebc171ebdd",
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8f54105b00>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84685c23e4eb899d7fed3a87b7f8915e",
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function that takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon (which we will pass later). Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Unlike numpy, PyTorch has no argmax function, but Google is your friend... Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "882f51819100c850120e73340aec387d",
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    with torch.no_grad():\n",
    "        scores = model(torch.Tensor(state))\n",
    "        if random.random() > epsilon:\n",
    "            m, am = scores.max(0)\n",
    "            return am.item()\n",
    "        return random.randint(0, scores.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21f939075cb0c8dde152dabf47568a9d",
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = select_action(model, s, 0.05)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d00ab2e5e0b39257771d0e778fda2d6",
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4839aac72a80552046ebecc40c1615cf",
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c466ee49add35cb1ec6a3e4a85f733c9",
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    values = model(state)\n",
    "    return torch.gather(values, 1, action.unsqueeze(-1)).reshape(-1)\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    # target = R + gamma * V(S')\n",
    "    next_values = model(next_state)\n",
    "    return reward + discount_factor * torch.max(next_values, 1)[0] * (1 - done.type(torch.FloatTensor)) \n",
    "    \n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "877c400001292b619e6871c1366524b9",
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59531170129776\n"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2057dee580a43fb0442fe52557c0ac64",
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06dd71aae5c3c699f2b707b348a88107",
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c3f61b2ca270d84ab9b28d989dd65d4c",
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        duration = 0\n",
    "        while not done:\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            a = select_action(model, s, epsilon)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            memory.push((s, a, r, s_next, done))\n",
    "            \n",
    "            train(model, memory, optimizer, batch_size, discount_factor)\n",
    "            \n",
    "            s = s_next\n",
    "            duration += 1\n",
    "            global_steps += 1\n",
    "            \n",
    "        env.close()\n",
    "        episode_durations.append(duration)\n",
    "        \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "episode_durations = run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70d16eb61eae34605e8d7813a70a604a",
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_durations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-88f369d76629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_durations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode durations per episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'episode_durations' is not defined"
     ]
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extra cell di Davide, To be removed!!\n",
    "\n",
    "# # Con 1000 episodi si vede qualcosa di più chiaro, proviamo anche a renderizzare gli ultimi 5 episodi\n",
    "# def run_episodes_and_render(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "#     optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "#     global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "#     episode_durations = []  #\n",
    "#     for i in range(num_episodes):\n",
    "#         s = env.reset()\n",
    "#         done = False\n",
    "#         duration = 0\n",
    "#         while not done:\n",
    "#             epsilon = get_epsilon(i)\n",
    "#             a = select_action(model, s, epsilon)\n",
    "#             s_next, r, done, _ = env.step(a)\n",
    "#             memory.push((s, a, r, s_next, done))\n",
    "            \n",
    "#             train(model, memory, optimizer, batch_size, discount_factor)\n",
    "            \n",
    "#             # visualizziamo gli ultimi 5 episodi\n",
    "#             if(i >= num_episodes-5):\n",
    "#                 env.render()\n",
    "#                 time.sleep(0.05)\n",
    "            \n",
    "#             s = s_next\n",
    "#             duration += 1\n",
    "            \n",
    "#         env.close()\n",
    "#         episode_durations.append(duration)\n",
    "        \n",
    "#     return episode_durations, memory\n",
    "\n",
    "# num_episodes = 1000\n",
    "# memory = ReplayMemory(10000)\n",
    "# model = QNetwork(num_hidden)\n",
    "\n",
    "# episode_durations, memory = run_episodes_and_render(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "\n",
    "# def smooth(x, N):\n",
    "#     cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "#     return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "# plt.plot(smooth(episode_durations, 10))\n",
    "# plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e106dba734da10d4d8b3bf90d6bb772",
     "grade": false,
     "grade_id": "cell-49e6bf74834a67ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.6 Semi-gradient vs. true gradient (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acf155c686f3916453a3d11d95994987",
     "grade": false,
     "grade_id": "cell-fc30be2a6983bc77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Note that by using automatic differentiation in PyTorch, it is (relatively) easy to implement the true gradient method. Hint: PyTorch may complain about computing gradients for the target in [smooth_l1_loss](https://pytorch.org/docs/stable/nn.html?highlight=smooth_l1_loss#torch.nn.functional.smooth_l1_loss). How can you circumvent this problem? Implement the `train_true_gradient` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d1e72257ed8c59175352e163f1bfdaf",
     "grade": true,
     "grade_id": "cell-71707640573b23d1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c3a4b9c5357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Returns a Python scalar, and releases history (similar to .detach())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed' is not defined"
     ]
    }
   ],
   "source": [
    "def train_true_gradient(model, memory, optimizer, batch_size, discount_factor):\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    # come fare in modo che il gradiente sia calcolato?\n",
    "    target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val - target, torch.zeros_like(q_val))\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "episode_durations_true_gradient = run_episodes(\n",
    "    train_true_gradient, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.plot(smooth(episode_durations_true_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Semi-gradient', 'True gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95b462060bc00fccd7e8bc2ccc857215",
     "grade": false,
     "grade_id": "cell-b6fb5a1b0894fb4e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Which algorithm performs better? Is this what you would expect? Can you explain this?\n",
    "\n",
    "Note: you may want to play around with the number of episodes to answer this question, but please reset it to 100 before handing in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b2e5712195d20cce7d1a6afb34e24a41",
     "grade": true,
     "grade_id": "cell-d99dae457ea5bde6",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de7203182e41f55f391af5892477e89d",
     "grade": false,
     "grade_id": "cell-6607b79e73a101a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Policy Gradient (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "951b88e9cd8396d088d3f80e6da9690c",
     "grade": false,
     "grade_id": "cell-083fe71da94aa7aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So we have spent a lot of time working on *value based* methods. We will now switch to *policy based* methods, i.e. learn a policy directly rather than learn a value function from which the policy follows. Mention two advantages of using a policy based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5c1f505cb22eca6eb3b8213ff23e60f",
     "grade": true,
     "grade_id": "cell-134510705650d5ac",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Some advantages of using policy based methods are:\n",
    "- easier to include prior knowledge\n",
    "- we directly optimize the quantities of interest\n",
    "- easier to ensure smooth changes of the policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "174629c02b62968e23fa6088c4d5763b",
     "grade": false,
     "grade_id": "cell-76a10fe31897025f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2bc16b45e6145226b8a6f5117003b7f5",
     "grade": false,
     "grade_id": "cell-34f0712f792bbcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to do so, we will implement a Policy network. Although in general this does not have to be the case, we will use an architecture very similar to the Q-network (two layers with ReLU activation for the hidden layer). Since we have discrete actions, our model will output one value per action, where each value represents the (normalized!) log-probability of selecting that action. *Use the (log-)softmax activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "155baf230fd6deb5f6ccf93138fa3419",
     "grade": false,
     "grade_id": "cell-6a31440f9477f963",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.do = nn.Dropout(p=0.6)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "        self.sm = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "#         x = self.do(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.sm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3cb94e04b03fa4b663bcf38a96ef656d",
     "grade": true,
     "grade_id": "cell-9d280fe6520edc91",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4578, 0.5422],\n",
      "        [0.4657, 0.5343],\n",
      "        [0.4563, 0.5437],\n",
      "        [0.4634, 0.5366],\n",
      "        [0.4564, 0.5436],\n",
      "        [0.4725, 0.5275],\n",
      "        [0.4769, 0.5231],\n",
      "        [0.4834, 0.5166],\n",
      "        [0.4797, 0.5203],\n",
      "        [0.4618, 0.5382]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "log_p = model(x)\n",
    "\n",
    "# Does the outcome make sense?\n",
    "print(log_p.exp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "619c714e930c0d167304597d188f229b",
     "grade": false,
     "grade_id": "cell-35294ca4eda15b11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Monte Carlo REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93ed9cbcf70541f5a04709ee89a16e78",
     "grade": false,
     "grade_id": "cell-44f33e587542974d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the *Monte Carlo* policy gradient algorithm. Remember from lab 1 that this means that we will estimate returns for states by sample episodes. Compared to DQN, this means that we do *not* perform an update step at every environment step, but only at the end of each episode. This means that we should generate an episode of data, compute the REINFORCE loss (which requires computing the returns) and then perform a gradient step.\n",
    "\n",
    "To help you, we already implemented a few functions that you can (but do not have to) use.\n",
    "\n",
    "* You can use `torch.multinomial` to sample from a categorical distribution.\n",
    "* The REINFORCE loss is defined as $- \\sum_t \\log \\pi_\\theta(a_t|s_t) G_t$, which means that you should compute the (discounted) return $G_t$ for all $t$. Make sure that you do this in **linear time**, otherwise your algorithm will be very slow! Note the - (minus) since you want to maximize return while you want to minimize the loss.\n",
    "* Importantly, you should **normalize the returns** (not the rewards!, e.g. subtract mean and divide by standard deviation within the episode) before computing the loss, or your estimator will have very high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b2c75181678fed25fcc7c8b39bb7de3",
     "grade": true,
     "grade_id": "cell-3f6e32c4931392bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state):\n",
    "    # Samples an action according to the probability distribution induced by the model\n",
    "    # Also returns the log_probability\n",
    "    \n",
    "    log_p = model(torch.FloatTensor(state).reshape(1, -1)).view(-1)\n",
    "    \n",
    "    action = torch.multinomial(torch.exp(log_p), 1).item()\n",
    "#     prob = torch.distributions.Categorical(torch.exp(log_p))\n",
    "#     action = prob.sample().item()\n",
    "    \n",
    "    return action, log_p[action]\n",
    "\n",
    "def run_episode(env, model):\n",
    "    \n",
    "    episode = []\n",
    "    \n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        a, log_p = select_action(model, s)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        \n",
    "        episode.append((s, a, log_p, s_next, r))\n",
    "        s = s_next\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def compute_reinforce_loss(episode, discount_factor):\n",
    "    # Compute the reinforce loss\n",
    "    # Make sure that your function runs in LINEAR TIME\n",
    "    # Don't forget to normalize your RETURNS (not rewards)\n",
    "    # Note that the rewards/returns should be maximized \n",
    "    # while the loss should be minimized so you need a - somewhere\n",
    "    \n",
    "    returns = []\n",
    "    log_ps = []\n",
    "    G = 0\n",
    "    for _, _, log_p, _, r in reversed(episode):\n",
    "        G = r + discount_factor * G\n",
    "        returns.append(G)\n",
    "        log_ps.append(log_p)\n",
    "        \n",
    "    log_ps = torch.stack(log_ps)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    returns -= returns.mean()\n",
    "    returns /= returns.std()\n",
    "    \n",
    "    loss = -1 * torch.sum(log_ps * returns)\n",
    "    return loss\n",
    "\n",
    "def run_episodes_policy_gradient(model, env, num_episodes, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        episode = run_episode(env, model)\n",
    "        loss = compute_reinforce_loss(episode, discount_factor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                           \n",
    "        if i % 20 == 0:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(i, len(episode), '\\033[92m' if len(episode) >= 195 else '\\033[99m'))\n",
    "        episode_durations.append(len(episode))\n",
    "        \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 19 steps\n",
      "\u001b[99m Episode 20 finished after 13 steps\n",
      "\u001b[99m Episode 40 finished after 14 steps\n",
      "\u001b[99m Episode 60 finished after 14 steps\n",
      "\u001b[99m Episode 80 finished after 23 steps\n",
      "\u001b[99m Episode 100 finished after 13 steps\n",
      "\u001b[99m Episode 120 finished after 34 steps\n",
      "\u001b[99m Episode 140 finished after 13 steps\n",
      "\u001b[99m Episode 160 finished after 29 steps\n",
      "\u001b[99m Episode 180 finished after 17 steps\n",
      "\u001b[99m Episode 200 finished after 21 steps\n",
      "\u001b[99m Episode 220 finished after 62 steps\n",
      "\u001b[99m Episode 240 finished after 81 steps\n",
      "\u001b[99m Episode 260 finished after 71 steps\n",
      "\u001b[99m Episode 280 finished after 41 steps\n",
      "\u001b[99m Episode 300 finished after 19 steps\n",
      "\u001b[99m Episode 320 finished after 72 steps\n",
      "\u001b[99m Episode 340 finished after 101 steps\n",
      "\u001b[99m Episode 360 finished after 87 steps\n",
      "\u001b[99m Episode 380 finished after 69 steps\n",
      "\u001b[92m Episode 400 finished after 200 steps\n",
      "\u001b[92m Episode 420 finished after 200 steps\n",
      "\u001b[92m Episode 440 finished after 200 steps\n",
      "\u001b[99m Episode 460 finished after 110 steps\n",
      "\u001b[92m Episode 480 finished after 200 steps\n",
      "\u001b[92m Episode 500 finished after 200 steps\n",
      "\u001b[92m Episode 520 finished after 200 steps\n",
      "\u001b[99m Episode 540 finished after 144 steps\n",
      "\u001b[92m Episode 560 finished after 200 steps\n",
      "\u001b[92m Episode 580 finished after 200 steps\n",
      "\u001b[92m Episode 600 finished after 200 steps\n",
      "\u001b[92m Episode 620 finished after 200 steps\n",
      "\u001b[92m Episode 640 finished after 200 steps\n",
      "\u001b[99m Episode 660 finished after 132 steps\n",
      "\u001b[99m Episode 680 finished after 186 steps\n",
      "\u001b[99m Episode 700 finished after 127 steps\n",
      "\u001b[92m Episode 720 finished after 200 steps\n",
      "\u001b[92m Episode 740 finished after 200 steps\n",
      "\u001b[92m Episode 760 finished after 200 steps\n",
      "\u001b[92m Episode 780 finished after 200 steps\n",
      "\u001b[92m Episode 800 finished after 200 steps\n",
      "\u001b[99m Episode 820 finished after 139 steps\n",
      "\u001b[92m Episode 840 finished after 200 steps\n",
      "\u001b[92m Episode 860 finished after 200 steps\n",
      "\u001b[92m Episode 880 finished after 200 steps\n",
      "\u001b[92m Episode 900 finished after 200 steps\n",
      "\u001b[92m Episode 920 finished after 200 steps\n",
      "\u001b[92m Episode 940 finished after 200 steps\n",
      "\u001b[92m Episode 960 finished after 200 steps\n",
      "\u001b[92m Episode 980 finished after 200 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd6ca6b5f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feel free to play around with the parameters!\n",
    "num_episodes = 1000\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.001\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "episode_durations_policy_gradient = run_episodes_policy_gradient(\n",
    "    model, env, num_episodes, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations_policy_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Policy gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9fe846472bc09094ba671593c4b40b4",
     "grade": false,
     "grade_id": "cell-af9c49b396393dc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Actor-Critic (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff32c0931b08aa9a5719639105a7b3e5",
     "grade": false,
     "grade_id": "cell-7eabad968ce02adf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will now implement the basic Actor-Critic algorithm, which means that instead of using Monte Carlo returns, we will bootstrap (1-step) returns using a critic (state-value function), so $G_t = R_t + \\gamma V(s_{t+1})$. What happens at the end of the episode? Hint: you may find it useful to have a look at the `train` method for DQN.\n",
    "\n",
    "* Note that we now have to train an actor (policy) and a critic (value network).\n",
    "* We will do this using a single optimizer, which means that we have to sum the loss for the actor and the critic into a single loss term. \n",
    "* For the critic, use the `smooth_l1_loss` like with DQN.\n",
    "* For the actor, the loss should be the REINFORCE loss, but with two differences:\n",
    "    - Instead of the Monte Carlo return $G_t$, use the one step return $G_{t:t+1}$ where the critic is used to bootstrap the value of $s_{t+1}$.\n",
    "    - Instead of normalizing the returns (which can be viewed as using the average as baseline and then scaling), we will use the estimated value $V(s_t)$ as baseline.\n",
    "* **Important**: note that you cannot use `with torch.no_grad():` to compute the critic value (for the current state) since you need gradients to train the critic! However, when using the value to compute the actor loss, you do not want to get gradients of the critic parameters w.r.t. the actor loss (e.g. your target and baseline must be constant)! Therefore, use `v.detach()` on the output of the critic when it is used in the loss term for the actor, this will make sure the value(s) are treated as a constant and no gradients will be backpropagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b649f137296d2c6e9ac367781f1b04e",
     "grade": true,
     "grade_id": "cell-5a7326fd2ab9349c",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, finished 0 / 10000 episodes, average episode duration of last 100 episodes: nan\n",
      "Step 100, finished 82 / 10000 episodes, average episode duration of last 100 episodes: 18.01219512195122\n",
      "Step 200, finished 178 / 10000 episodes, average episode duration of last 100 episodes: 16.59\n",
      "Step 300, finished 273 / 10000 episodes, average episode duration of last 100 episodes: 17.42\n",
      "Step 400, finished 353 / 10000 episodes, average episode duration of last 100 episodes: 18.91\n",
      "Step 500, finished 401 / 10000 episodes, average episode duration of last 100 episodes: 24.83\n",
      "Step 600, finished 444 / 10000 episodes, average episode duration of last 100 episodes: 33.29\n",
      "Step 700, finished 478 / 10000 episodes, average episode duration of last 100 episodes: 37.95\n",
      "Step 800, finished 509 / 10000 episodes, average episode duration of last 100 episodes: 43.04\n",
      "Step 900, finished 542 / 10000 episodes, average episode duration of last 100 episodes: 46.68\n",
      "Step 1000, finished 571 / 10000 episodes, average episode duration of last 100 episodes: 50.91\n",
      "Step 1100, finished 596 / 10000 episodes, average episode duration of last 100 episodes: 53.57\n",
      "Step 1200, finished 619 / 10000 episodes, average episode duration of last 100 episodes: 55.54\n",
      "Step 1300, finished 638 / 10000 episodes, average episode duration of last 100 episodes: 62.53\n",
      "Step 1400, finished 661 / 10000 episodes, average episode duration of last 100 episodes: 68.16\n",
      "Step 1500, finished 687 / 10000 episodes, average episode duration of last 100 episodes: 69.94\n",
      "Step 1600, finished 706 / 10000 episodes, average episode duration of last 100 episodes: 72.47\n",
      "Step 1700, finished 722 / 10000 episodes, average episode duration of last 100 episodes: 76.07\n",
      "Step 1800, finished 738 / 10000 episodes, average episode duration of last 100 episodes: 78.31\n",
      "Step 1900, finished 755 / 10000 episodes, average episode duration of last 100 episodes: 81.51\n",
      "Step 2000, finished 769 / 10000 episodes, average episode duration of last 100 episodes: 86.62\n",
      "Step 2100, finished 790 / 10000 episodes, average episode duration of last 100 episodes: 93.1\n",
      "Step 2200, finished 802 / 10000 episodes, average episode duration of last 100 episodes: 95.53\n",
      "Step 2300, finished 814 / 10000 episodes, average episode duration of last 100 episodes: 101.65\n",
      "Step 2400, finished 825 / 10000 episodes, average episode duration of last 100 episodes: 105.92\n",
      "Step 2500, finished 837 / 10000 episodes, average episode duration of last 100 episodes: 109.59\n",
      "Step 2600, finished 846 / 10000 episodes, average episode duration of last 100 episodes: 115.58\n",
      "Step 2700, finished 856 / 10000 episodes, average episode duration of last 100 episodes: 122.27\n",
      "Step 2800, finished 863 / 10000 episodes, average episode duration of last 100 episodes: 126.79\n",
      "Step 2900, finished 876 / 10000 episodes, average episode duration of last 100 episodes: 135.77\n",
      "Step 3000, finished 891 / 10000 episodes, average episode duration of last 100 episodes: 141.99\n",
      "Step 3100, finished 897 / 10000 episodes, average episode duration of last 100 episodes: 145.92\n",
      "Step 3200, finished 908 / 10000 episodes, average episode duration of last 100 episodes: 152.68\n",
      "Step 3300, finished 918 / 10000 episodes, average episode duration of last 100 episodes: 154.14\n",
      "Step 3400, finished 926 / 10000 episodes, average episode duration of last 100 episodes: 156.63\n",
      "Step 3500, finished 937 / 10000 episodes, average episode duration of last 100 episodes: 160.39\n",
      "Step 3600, finished 946 / 10000 episodes, average episode duration of last 100 episodes: 161.89\n",
      "Step 3700, finished 955 / 10000 episodes, average episode duration of last 100 episodes: 162.85\n",
      "Step 3800, finished 964 / 10000 episodes, average episode duration of last 100 episodes: 162.27\n",
      "Step 3900, finished 971 / 10000 episodes, average episode duration of last 100 episodes: 163.45\n",
      "Step 4000, finished 981 / 10000 episodes, average episode duration of last 100 episodes: 169.25\n",
      "Step 4100, finished 992 / 10000 episodes, average episode duration of last 100 episodes: 171.71\n",
      "Step 4200, finished 1000 / 10000 episodes, average episode duration of last 100 episodes: 172.27\n",
      "Step 4300, finished 1011 / 10000 episodes, average episode duration of last 100 episodes: 169.6\n",
      "Step 4400, finished 1017 / 10000 episodes, average episode duration of last 100 episodes: 171.84\n",
      "Step 4500, finished 1029 / 10000 episodes, average episode duration of last 100 episodes: 173.52\n",
      "Step 4600, finished 1033 / 10000 episodes, average episode duration of last 100 episodes: 175.47\n",
      "Step 4700, finished 1045 / 10000 episodes, average episode duration of last 100 episodes: 178.36\n",
      "Step 4800, finished 1049 / 10000 episodes, average episode duration of last 100 episodes: 178.55\n",
      "Step 4900, finished 1063 / 10000 episodes, average episode duration of last 100 episodes: 179.99\n",
      "Step 5000, finished 1071 / 10000 episodes, average episode duration of last 100 episodes: 176.31\n",
      "Step 5100, finished 1080 / 10000 episodes, average episode duration of last 100 episodes: 175.39\n",
      "Step 5200, finished 1095 / 10000 episodes, average episode duration of last 100 episodes: 177.19\n",
      "Step 5300, finished 1099 / 10000 episodes, average episode duration of last 100 episodes: 175.33\n",
      "Step 5400, finished 1112 / 10000 episodes, average episode duration of last 100 episodes: 176.99\n",
      "Step 5500, finished 1116 / 10000 episodes, average episode duration of last 100 episodes: 176.53\n",
      "Step 5600, finished 1130 / 10000 episodes, average episode duration of last 100 episodes: 175.66\n",
      "Step 5700, finished 1132 / 10000 episodes, average episode duration of last 100 episodes: 175.66\n",
      "Step 5800, finished 1146 / 10000 episodes, average episode duration of last 100 episodes: 175.62\n",
      "Step 5900, finished 1149 / 10000 episodes, average episode duration of last 100 episodes: 174.98\n",
      "Step 6000, finished 1162 / 10000 episodes, average episode duration of last 100 episodes: 176.77\n",
      "Step 6100, finished 1165 / 10000 episodes, average episode duration of last 100 episodes: 177.54\n",
      "Step 6200, finished 1178 / 10000 episodes, average episode duration of last 100 episodes: 183.74\n",
      "Step 6300, finished 1181 / 10000 episodes, average episode duration of last 100 episodes: 184.51\n",
      "Step 6400, finished 1194 / 10000 episodes, average episode duration of last 100 episodes: 188.06\n",
      "Step 6500, finished 1197 / 10000 episodes, average episode duration of last 100 episodes: 188.67\n",
      "Step 6600, finished 1210 / 10000 episodes, average episode duration of last 100 episodes: 193.58\n",
      "Step 6700, finished 1215 / 10000 episodes, average episode duration of last 100 episodes: 192.82\n",
      "Step 6800, finished 1229 / 10000 episodes, average episode duration of last 100 episodes: 194.3\n",
      "Step 6900, finished 1239 / 10000 episodes, average episode duration of last 100 episodes: 189.38\n",
      "Step 7000, finished 1245 / 10000 episodes, average episode duration of last 100 episodes: 188.15\n",
      "Step 7100, finished 1256 / 10000 episodes, average episode duration of last 100 episodes: 187.8\n",
      "Step 7200, finished 1261 / 10000 episodes, average episode duration of last 100 episodes: 187.72\n",
      "Step 7300, finished 1272 / 10000 episodes, average episode duration of last 100 episodes: 187.16\n",
      "Step 7400, finished 1277 / 10000 episodes, average episode duration of last 100 episodes: 187.02\n",
      "Step 7500, finished 1288 / 10000 episodes, average episode duration of last 100 episodes: 186.51\n",
      "Step 7600, finished 1293 / 10000 episodes, average episode duration of last 100 episodes: 186.71\n",
      "Step 7700, finished 1304 / 10000 episodes, average episode duration of last 100 episodes: 187.55\n",
      "Step 7800, finished 1309 / 10000 episodes, average episode duration of last 100 episodes: 187.63\n",
      "Step 7900, finished 1320 / 10000 episodes, average episode duration of last 100 episodes: 188.68\n",
      "Step 8000, finished 1325 / 10000 episodes, average episode duration of last 100 episodes: 188.69\n",
      "Step 8100, finished 1336 / 10000 episodes, average episode duration of last 100 episodes: 194.73\n",
      "Step 8200, finished 1343 / 10000 episodes, average episode duration of last 100 episodes: 195.69\n",
      "Step 8300, finished 1353 / 10000 episodes, average episode duration of last 100 episodes: 195.24\n",
      "Step 8400, finished 1361 / 10000 episodes, average episode duration of last 100 episodes: 194.1\n",
      "Step 8500, finished 1369 / 10000 episodes, average episode duration of last 100 episodes: 194.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8600, finished 1378 / 10000 episodes, average episode duration of last 100 episodes: 193.51\n",
      "Step 8700, finished 1386 / 10000 episodes, average episode duration of last 100 episodes: 193.29\n",
      "Step 8800, finished 1394 / 10000 episodes, average episode duration of last 100 episodes: 193.24\n",
      "Step 8900, finished 1402 / 10000 episodes, average episode duration of last 100 episodes: 193.24\n",
      "Step 9000, finished 1411 / 10000 episodes, average episode duration of last 100 episodes: 192.56\n",
      "Step 9100, finished 1418 / 10000 episodes, average episode duration of last 100 episodes: 191.8\n",
      "Step 9200, finished 1428 / 10000 episodes, average episode duration of last 100 episodes: 191.23\n",
      "Step 9300, finished 1437 / 10000 episodes, average episode duration of last 100 episodes: 188.48\n",
      "Step 9400, finished 1448 / 10000 episodes, average episode duration of last 100 episodes: 186.98\n",
      "Step 9500, finished 1454 / 10000 episodes, average episode duration of last 100 episodes: 186.32\n",
      "Step 9600, finished 1465 / 10000 episodes, average episode duration of last 100 episodes: 187.34\n",
      "Step 9700, finished 1472 / 10000 episodes, average episode duration of last 100 episodes: 187.36\n",
      "Step 9800, finished 1482 / 10000 episodes, average episode duration of last 100 episodes: 187.31\n",
      "Step 9900, finished 1488 / 10000 episodes, average episode duration of last 100 episodes: 187.94\n"
     ]
    }
   ],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "def select_action(model, state):\n",
    "    # Samples an action according to the probability distribution induced by the model\n",
    "    # Also returns the log_probability\n",
    "    \n",
    "    log_p = model(torch.FloatTensor(state))\n",
    "    \n",
    "    action = torch.multinomial(torch.exp(log_p), 1)\n",
    "    \n",
    "    log_p = log_p.gather(1, action).squeeze()\n",
    "    action = action.squeeze()\n",
    "    \n",
    "    # action and log_p should be a 1 dimensional vector\n",
    "    n = len(state)\n",
    "    assert action.size() == (n, )\n",
    "    assert log_p.size() == (n, )\n",
    "    return action, log_p\n",
    "\n",
    "def train_actor_critic(actor, critic, optimizer, log_ps, state, reward, next_state, done, discount_factor):\n",
    "    \n",
    "    value = critic(state)\n",
    "    \n",
    "    # target = R + gamma * V(S')\n",
    "    dones = done.type(torch.FloatTensor).view(-1)\n",
    "    reward = reward.view(-1)\n",
    "    next_values = critic(next_state)\n",
    "    target = reward + discount_factor * next_values * (1 - dones)\n",
    "    \n",
    "    err = target - value\n",
    "    value_loss = F.smooth_l1_loss(err, torch.zeros_like(err))\n",
    "    \n",
    "    # instead of \"target\", we use \"err = target - value\" since the value of the\n",
    "    # current state is used as baseline\n",
    "    actor_loss = -1 * (err.detach() * log_ps)\n",
    "    actor_loss = actor_loss.mean() \n",
    "    \n",
    "    # The loss is composed of the value_loss (for the critic) and the actor_loss\n",
    "    loss = actor_loss + value_loss*0.5\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 1)\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), value_loss.item(), actor_loss.item()  # Returns a Python scalar, and releases history (similar to .detach())\n",
    "\n",
    "def run_episodes_actor_critic(actor, critic, envs, max_episodes, max_steps, discount_factor, actor_learn_rate, critic_learn_rate):\n",
    "    \n",
    "    # We can use a single optimizer for both the actor and the critic, even with separate learn rates\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': actor.parameters(), 'lr': actor_learn_rate},\n",
    "        {'params': critic.parameters(), 'lr': critic_learn_rate}\n",
    "    ])\n",
    "    \n",
    "    episode_durations = []\n",
    "    state = torch.tensor([env.reset() for env in envs], dtype=torch.float)\n",
    "    current_episode_lengths = torch.zeros(len(envs), dtype=torch.int64)\n",
    "    step_losses = []  # Keep track of losses for plotting\n",
    "    for i in range(max_steps):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Step {i}, finished {len(episode_durations)} / {max_episodes} episodes, average episode duration of last 100 episodes: {np.mean(episode_durations[-100:])}\")\n",
    "        \n",
    "        action, log_ps = select_action(actor, state)\n",
    "        next_state, reward, done, _ = zip(*[env.step(a.item()) for env, a in zip(envs, action)])\n",
    "        \n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "        current_episode_lengths += 1\n",
    "        \n",
    "        losses = train_actor_critic(actor, critic, optimizer, log_ps, state, reward, next_state, done, discount_factor)\n",
    "        \n",
    "        step_losses.append(losses)\n",
    "        \n",
    "        # Reset envs that are done\n",
    "        next_state = torch.tensor([\n",
    "            env.reset() if d else s.tolist()\n",
    "            for env, s, d in zip(envs, next_state, done)\n",
    "        ], dtype=torch.float)\n",
    "        \n",
    "        episode_durations.extend(current_episode_lengths[done])\n",
    "        current_episode_lengths[done] = 0  # PyTorch can also work in place\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Check if we have finished sufficiently many episodes\n",
    "        if len(episode_durations) >= max_episodes:\n",
    "            break\n",
    "        \n",
    "    return episode_durations[:max_episodes], step_losses  # In case we want exactly num_episodes returned\n",
    "\n",
    "\n",
    "num_envs = 16\n",
    "max_steps = 10000\n",
    "max_episodes = 10000\n",
    "discount_factor = 0.8\n",
    "lr_actor = 5e-4\n",
    "lr_critic = 5e-4\n",
    "seed = 42\n",
    "\n",
    "actor = PolicyNetwork(num_hidden)\n",
    "critic = ValueNetwork(num_hidden)\n",
    "\n",
    "envs = [gym.envs.make(\"CartPole-v0\") for i in range(num_envs)]\n",
    "\n",
    "for i, env in enumerate(envs):\n",
    "    env.seed(seed + i)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "episode_durations, step_losses = run_episodes_actor_critic(actor, critic, envs, max_episodes, max_steps, discount_factor, lr_actor, lr_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd8W+W9+PHP13uPJM4ezoaQTQg7hLITWlZZ7S2j0ACX3kt/FG6h0Em5pS2lLR0UWvYFCmWXhEKgzBQCGWTv7cRJnMR7SJb0/P4458hHsmzLlmzL9vf9evmVo+cMPVbkrx59zzPEGINSSqneK6m7K6CUUqpzaaBXSqleTgO9Ukr1chrolVKql9NAr5RSvZwGeqWU6uU00KuEISJvisjVcb7mj0Xk/+J0rSdE5GfxuFaUz/d1EXm7q55P9V4a6FVcichOEakXkRrXzx+iOdcYc54x5snOrmMiEpFiETEikuKUGWOeMcac3Z31Ur1DStuHKNVuXzbGvNPdlUgkIpJsjPF3dz1U36QtetVlROQaEVkiIr8XkUoR2SgiZ7j2vy8i19vb40TkA/u4QyLyvOu4k0Tkc3vf5yJykmvfaPu8ahFZDAwIq8MJIvJvEakQkVUiMreV+s4QkRX2tZ4HMsJ+l4/DjjciMs7efkJEHhKRRSJSC5wuIvNFZKWIVInIHhH5sev0D+1/K+xvQSeGP0cbv/f7InKP/fpWi8jbIjLA3pchIv8nIoft3/tzERnUyn+V6mU00KuudjywHSsA/wh4WUT6RTjuHuBtoBAYDvwewD52IfAg0B94AFgoIv3t854FltvXvwcI5vxFZJh97s+AfsBtwEsiUhT+5CKSBrwKPG0f+3fgknb+rl8D7gVygY+BWuAqoACYD9wkIhfax86x/y0wxuQYYz4Jq09bv7fzfNcCA4E0+/fDfg3ygRH2uTcC9e38XVQPpoFedYZX7Zaj8/Mt176DwG+NMY3GmOeBTVhBL1wjMAoYaoxpMMY4Ldv5wBZjzNPGGJ8x5jlgI/BlERkJHAf8wBjjMcZ8CPzDdc3/ABYZYxYZYwLGmMXAMmBehOc/AUh11fVF4PN2vg6vGWOW2M/VYIx53xizxn68GngOOC3Ka7X4e7uOedwYs9kYUw+8AEy3yxuxAvw4Y4zfGLPcGFPVzt9F9WAa6FVnuNAYU+D6+Ytr314TOpPeLmBohGv8DyDAZyKyTkS+aZcPtc9x2wUMs/eVG2Nqw/Y5RgGXuj+EgFOAIRGef2gLdW2PPe4HInK8iLwnImUiUonVsh4Q+dSI9Wnp93bsd23XATn29tPAW8DfRGSfiPxSRFKj/SVUz6eBXnW1YSIirscjgX3hBxlj9htjvmWMGQrcAPzJzn/vwwrYhF1jL1AKFIpIdtg+xx7g6bAPoWxjzH0R6lnaQl0dtUCW80BEBke4RvjUsM8CrwMjjDH5wJ+xPswiHRuutd+7VfY3kp8YYyYBJwHnY6WQVB+hgV51tYHAf4tIqohcChwNLAo/SEQuFZHh9sNyrEDot4+dICJfE5EUEbkcmAS8YYzZhZWK+YmIpInIKYSmNv4PK8Vzjogk2zcp57qex+0TwGfXNUVELgZmu/avAo4RkekikgH8OIrfPRc4YoxpEJHZWDl1RxkQAMa0cG6Lv3dbTyoip4vIFBFJBqqwUjnaA6gP0UCvOsM/JLQf/SuufUuB8cAhrBuVXzXGHI5wjeOApSJSg9UKvsUYs8M+9nzgu8BhrBTP+caYQ/Z5X8O64XsE62bvU84FjTF7gAuA72MF1j3A7UT4OzDGeIGLgWuwPmguB1527d8M/BR4B9iCdbO1Lf8J/FREqoEfYuXRnevV2a/HEjutdEJYfdr6vVszGHgRK8hvAD7A+tBTfYTowiOqq4jINcD1xphTursuSvUl2qJXSqleTgO9Ukr1cpq6UUqpXk5b9Eop1cslxKRmAwYMMMXFxd1dDaWU6lGWL19+yBjTbAqPcAkR6IuLi1m2bFl3V0MppXoUEYlqtLambpRSqpfTQK+UUr2cBnqllOrlNNArpVQvp4FeKaV6uTYDvYiMsOfQ3mDPC36LXd5PRBaLyBb730K7XETkQRHZKiKrRWRmZ/8SSimlWhZNi94HfNcYczTWqjs3i8gk4A7gXWPMeOBd+zHAeVizE44HFgAPxb3WSimlotZmP3pjTCnWIgwYY6pFZAPWqjYXAHPtw54E3ge+Z5c/Za/M86mIFIjIEPs6SinF5zuP8NHmMgCSk5K48vgRDMzNaOOs9mn0B3js4x3UenzN9iUnJfHlaUMYU5QT4czWrdtXyeqSSq44bgSh69IkrnYNmBKRYmAG1pzig5zgbYwpFZGB9mHDCF1CrcQuCwn0IrIAq8XPyJHuhXuUUr3B2+v2s+Dp5Xxw+1xG9W9a9OtIrZdL/2ytfS4CxsCWg9X84WvxzfL+5zMrWLz+QPB53IyB37yzmZz0FJ65/nimjSiI6pqBgGH+g9bSA6eMG8CIflltnJEYog70IpIDvAR8xxhT1conWaQdzWZOM8Y8AjwCMGvWLJ1ZTalutq+injteXsNJY/tz42ljY77e66usFSJXlVQGA31FnZcrHrGC/I+/PIlrTh7NzHsW88bqUn53hSE5KX4t5NUlFaQkCZt+dl6z624rq+GVFXv5w3tb+XT74agDfXVD07eDyvpGRsRYR3/AIEBSHH/vSKLqdWMvJPwS8Iwxxlll54CIDLH3DwEO2uUlEPL7DyfCmqBKqcTyjUeX8uHmMu57c2NcrpdkNwbdM+T+Y9U+Nh+oYcyAbK45eTQAN9kfKk7rO1a1Hh8/eHUtB6o83HDamIgfHmOLcrjtnInkZqSwr6I+6mtvO1QT3K5qaIypngtXlzL2+4v4r+dWxnSdaETT60aAR4ENxpgHXLteB662t68GXnOVX2X3vjkBqNT8vFKJb1tZbXDb6wvEfD0nvvoDTYH+4Q+3IwLv3HpasOzqk4oB+MfqfRyoaujQc9V7/Xz72RXc8PQyjvnRWzz96S6G5Gdw3uQhrZ5XmJVGeV10AXvt3kou/tO/g4+/+8KqDtXVsXpvBQDnT229jvEQTermZOAbwBoR+cIu+z5wH/CCiFwH7AYutfctAuYBW4E64Nq41lgpFXc+f2hg31tRz+gB2S0cHR2nRV/faK1D7vUFKCmvpzArNSRVkZaSxLTh+SxcXUqD18/Xjh+J1xfgvCnRB8CHPtjGG6ub2pO/vGQqF84YRlpK623Zwuw0yuu8UT3Hc5/tBuC3l0/ney+tprSygUDAdCjtUlHn5eEPtjMgJ71dv2dHtdmiN8Z8bIwRY8xUY8x0+2eRMeawMeYMY8x4+98j9vHGGHOzMWasMWaKMUanpVQqwVXZuedzjhkEwLKdR2K+ZnqqFV4een8bYLW6Ab79pfHNjn3quuM58+iBvLvxINc9uYybnlnBun2VfPn3H7N0e6S140P9a2NT2uemuWO57LgRbQZ5gMKsVCraaNEfrvHw0ZYynlm6m8F5GVw4Yxi3nzMRgOoIPXraEggYzv+9dUP3UI2n3ed3hI6MVUpx6wvWl/VjRxUCcPuLq1m7tzKma67dWwVASXk9JeV11DVaQTErLbnZsfmZqdx42lgKs1KDZfMf/Jg1eyv52+d7mh0fztPY9I1kaEFm1HUszEpj3b6Wf88Xlu3h2J+9wyMfbgfgqetmB88D+GSb9SFUWddIIBBdn5IL/riEknLrvsAtZzT/0OsMGuiVUpSU15OWksTXjx/FvRdNBmDj/uqYrunumDfvdx9RY39ryExtHugBZhX3Y+UPz+bFG08MKW8tEIN1U3TLwRq+PG0oD1w2jYtnDIu6jtnpyQQMbC+ribj/f15cDcBHWw5R3D+LCYNyATh1/AAA7nx5NZc//AnTfvo2P39zQ1TPuW5fJbkZKbxz6xwN9EqprnH3q2vYerCGK48bQXZ6ChfPGA7AweqO3Rh17K9s4PypQ5g3ZTBVDT7O+s2HAGRGaNG7ZaeH3jqs9fhbPf4Hr64FYMuBai6eObzZ+a356rFWB8F1+6qa7TscllZxp5wG5mVw17yjGdU/m6U7rDTXXz7awZYDrX84en0BAgZumDOGcQNzO71bpUMDvVJ93Gd2oLrK7v2SYefWGxo73vOmrNrDwWoPuRkpPHDZ9GAPnFH9s5g8LL/Vc8cNzOHSY4fz3m1z+cYJo6jztp4HX76rHKBDff+PHmK10P/ruZWs3F0esu+9TWXB7bkTi/jqscND9n9rzphmLfKnPml5wafLH/6Ec39rfdhlpXXt4n4JsZSgUqr7VDf4uPTY4Yy1pwMQEdJSkvD4Wm9Jt6as2moNTx6WT0ZqMp/eeQYHqz0cMzSvzWkDUpOT+NWl0wCrdd9ai351SQUl5fVcc1IxF7YjZeNIT0nmljPG87t3t7BmbyUzRhYG9+06XEuSwBPXzub4Mf0inn/q+AHMHFnAit1WV8naFj6Uymu9wZY/wP4OdiPtKG3RK9VH7ThUy38/t5JDNR7yMlND9mWkJIXc4GyvBvtDYph9Y3RgXgaTh+W3e26YnPRkvP5AxH79xhhu+r8VABw/OnIgjsZNc61vAjVhPWgOVnkYkJPOnAlFpKdETjelJCfx5Ddn861TRzO7uB9rSiLfT7j/7U0hj52xA11FA71SfZAxhvvf3sTrq/YxekA2p9g3Fx3pqckxteidD4mWAmS0nBRHpPTNm2v3s9ce1RpLX/R0uxvmniOhI2SP1Hnpl53W5vm5GancNX8S4wflcKQ2cp/8T1xdRL937lHBD8CuooFeqT7oH6tLWWgPMFr036dy+sSBIfszUuPTonf60ndUjn1jNby1DbDrcB0ARbnpMT2H8y3juc92h4ziPVIbXaB3pKck42lhRLHT9fKdW+cEv0F0JQ30SvVCNR5fyBwz4bYebOpOmJLcPAxkp6VEPWI0EudDIiPGFr3TgyZSnr7Crt/LN50U03MAwflwXlm5F4B/ri1l+a7y9gX61KSIKabqhkZ2Hq5j3pTBjBuYG3NdO0IDvVK9iDGGdfsqmfyjt/jrRzs6fJ2jBufG1I/eCcKxtuiz0q0Pikg3OXcermXcwJy4TBX83nfnAlBqp4JutHP/7WvRJ+H1B5oNnHJ6NYWnhrqSBnqlepFnP9sdnC/9rXX7WzxufRuDkIYVZlJa2UBDY/R5+pLyOnz+AO9tPMgdL69BhJhz0TnBFn3k1M2oOM0HP7J/Ftlpyc0mODtr0qCor+FMueD1R07f3D3/6I5XMEYa6JXqRd539f1ufa4XK1VxdguBrCjHynt/tOVQVM9bVu3hlF+8x0/fWM8DizcD8PB/HEtGC6Ngo5WdFjnQG2PYebg2ZEGTWBVkpVFR7w0Oepo2ooBTxxdFfb5z47nOG/rh6EzqVtiObwfxpoFeqV7EnZZPbyXQ+wMBpgzL55GrZkXcf/Yxg4Gm/vBtufkZK9Xx1Ce7WLO3kqLc9OA1YtF0MzY0eN73z400NAaCA57ioSArlS0HaoIjeMe1c5nBHDvN5Mx/43ACf0tTP3QFDfRKdYNN+6u56E9LeGVlSVyv2+hKG7Q21YDHFwiOgI3E6ckSbaD/LGy2y+e+dUJU57XFydG7u1fuq6jn4Q+sScbmTIi+xd2WgqxU1rgmcvvK9KHtOv8r06wBW0/8O/TeiDNrZ6TJ3LqKjoxVqhss3XGYlbsrWLm7gotmDG/7hCh9sLkpddPajUSPL9BqCzM1OYnM1GRqPO1fRem2sycwbmD7F92OJFL3ytJK66bm1OH5DIyxa6VbQab1euWmp7DmJ+e0+/zMtGSmDc9nVdigqbpgoO++cKsteqW6QXltUwDt6GpOWw9Wc/r97wdb3eGLh6RG6Dbp8Pj8raZ2AHIyUiL2X2/Lf5wwqt3ntCQ9JYnkJAnJ0R+usXr03HvhlHaPtG1NgT1F8sC8jn94TBtRgNcXCJnP3/k20to3qM6mgV6pblBR74243R6/e3crOw7VBlvxj35spQyuOamYgqxUHl+ys8W+9J7GQJtdH3PTU0IWw25NXkYK15xUzI6fz6MgK343HUWErLTkkH70zs1NJ60TL06g75/T8UC/sdS6kfvTN9YD8PnOI/z+X1sB4vqh1F4a6JXqBu5ui1X17UuPBAKGP/xrCzsPWWu8OsH85/ai3iXldYwotLod1nojd49s8PnbnJ4g2ha9MYY6r5/s9OROCWY56SkhLXrntYv3zc3x9mCmHYdq2ziyZacfZY0wdr6lvbfxYOwVi4NoFgd/TEQOishaV9nzIvKF/bPTWUtWRIpFpN6178+dWXmleiqvr6ml3d7pgDcdqOb+tzcHbxze/uLqkLnj754/ia8dPxKwRmVGUlHbGGzBtiQnPSW4WEhrPL4AvoDptBx0dnpKyICp+k7qxeLM93PimP4dvsYNc8Ywsl9W8N7Cn+xlFLtbNC36J4Bz3QXGmMud9WOBl4CXXbu3udaWvTF+VVWq93D3jmlpUNI/1+7ngj8uaZZ7D0RIxzxqj4JNSRKKB2STl2EFcWfJOjePz0+1x0e/NlIsuVG26J2bjdmd1KskOz0lpHtlndOij/PzDchJ55X/PIlfXDK1w9dIShImDMqlzusPTriWCKJZHPxDIOJKwWJ9T7sMeC7O9VKqVwsN9JFb9Pe9uYFVeyq4+9W1FN+xMJgOiHT8w/aapotuORWAwflWnjl8MQ0guBh2v5zWA31OempUOXonrdKelZ3aIzstOSR1U+/1kyStjxPoqBkjC2P+AMlKS6bO62O9a9WqAW281p0t1lfqVOCAMWaLq2y0iKwUkQ9E5NSWThSRBSKyTESWlZWVtXSYUr2Su6dNSy16Z454Z3FsJ9i1Ni2Bs3jIMUOtVZwa/c1b/06vlXi16J1jcjor0Ifl6KvqG8nNSO3Wm5utsQK9PzjN8wXTh/JSHCZei0Wsgf5KQlvzpcBIY8wM4FbgWRHJi3SiMeYRY8wsY8ysoqL4DXpQqifw+gPBVEdDC/O+Z4flvBsD1odDpHlfAIr7ZwVnYUxPSSJJmvLZbs6slG1N2JWTntLmLJjgCvQZnRPoc8Jy9FUNPvIyE3cIUFZaCvVef3AGz1vPmhDXqRo6osOBXkRSgIuB550yY4zHGHPY3l4ObAMmxFpJpXqbRn+AXDuP3tK87+EjKT2NARoa/Sx4enmwbFhBZnDiLfd0w1a3xJRm864AHK6NMtBnpOAPmDZvFtd0cuomvHtlVX0juemt30juTllpydR6fcG56WNdfCUeYmnRnwlsNMYEx3CLSJGIJNvbY4DxwPbYqqhU73Hr819QfMdCPt1+hLIaa6BTSy368AFPXn8gZB55gOkjCrjUXrQ6PA9c4/Hx2JLQ4fhl1R6e/3w3EF2LHqAyrPtnZV0je47UNT2PncfP7aRA73yzcFQ1NCZ0iz4zLZmAaerx1Bn3Etormu6VzwGfABNFpERErrN3XUHzm7BzgNUisgp4EbjRGBPxRq5SfdHL9sIWAOdOtib9aqnF/OmO0MmxPI0BfvWWtfbo09fN5uvHj+S2cyYGA/HgvIyI13GvmnTXK2tYsvUwA3PT2xzYNKbISjdsPhA6L/3dr63l1F++F0wLdfrN2PQUvL5A8AZ2dYMv+G0oETkpuUP2B3msc/LHQ5v/M8aYK1sovyZC2UtY3S2VUm345SVTWbi6tMWbqxVhc6N7/YHgKNjs9BTuvWgK4Ew4DFeFLTh917yjuXfRBmq9vmB3y/I6LwVZqfzrtrnBfH5Liu28cngXzX+s2gfA4VoPw9Oy2FZWE6xTZ3Cuu7G0minD8+1An7gt+vWlVm+bv9hdXtNamYqiq3R/DZTqo7LSkhEBT5SLe1zy0L+D22NdU+gWD8hm533zmTmyMOT4DLtl6b4H0NAYYMaIgqh6yBTlpiMCB6qaBmO5Z7N00ilOQOusfvSnjLMGMt30jHVvoqqhMfjBlYjOmhQ6PXOkpRq7WvfXQKk+SkRISRJW7K6I6ngnBXPsqELyM9sOdOl2gPH4QueJibafeGpyErnpKcHU0NLthznu3neC+2s9PqpcI287K6BNHJzL+VOHUFJejz9gqPEkdov+zKMHtn1QF9NAr1QXCp92oNFvQoKlI3zd0U/vPCO4HW1/dSc3/P6mMnYdtuZvaWj0t2vB7vys1OBcPEvCFtSobvBx7xsbADiuuLDZufE0ZoCVRlqy9RDGkNCBPhH792ugV6oLhYeAM48eiC/CoCanz/yZRw/i6etm09/VoybaIOf09rj71bWc9qv3afQHKCmvD6Z0opGanMTLK/ey63BtcAUlR63HT5IdQX524ZSor9kREwdbw3H+ZU8Slsg3Y4HgAKm5ExNjjJAGeqW6wflThwBWIG2MsJi0M6J19uhCTh1fRGpyEkcPsYJdtIG+qj50YJXTJbI93f22l1nfBG5/cXWwG+Xs0f0AeGlFCQ2NAUb0y2Ti4Pgt6RfJeZMHk5acFPxmksgterDSaxvvOZe/trBUY1fTQK9UF/IFDGdNGsSvL5sGWAt4eyMFenuwjbsvvdNPPtrWbPjkZ1/69QcAzC7u1+56J4nVnz4vI4W/2MHrXxsPcqTWS2Ec559v8fmThGGFmWyw53tP9BY9QEZqckLciAUN9Ep1KU9jgHEDc4KjJdOSkyKuMOWkbtyB3mmJR5ujv+TYyEsUdqRfd35mKqtKKsnPSg15/vK6rgn0AMMLM9lv9wBK9BZ9otFAr1QX8fkDeP2ha7WmpkQO9E6XSHcfbOf+bLRBLjU5KTjoyS0tOfoc/b0XTQbgrXUH+GJPBXuO1JOcJFwy0/oQWb+vqs0RtvEyol9WcDtPA327aKBXqos02AHdvXZoWnLk1M0KZ3ph191bp0UfPgdOa175z5P53RXTQ8ra06L/+vGjOGls84U4xg60PkB8AdNlLfqJg5ruA/SE1E0i0UCvVBeJtAReegsteqcnznGufPrU4QWAldePVn5marOBVO2deyVSqsjdj3/CoJxm+zvD5GH5wW1N3bSPvlpKdRFnbpgMV6DPSkvBY8/j4s7HO618d+v/W6eOZkh+RnCOnGiFp1ba80EBoa3nU+3l9txlY4q6JtDPGFEQ3I73MoK9nQZ6pbpIQ4Ql8PplWwGzoq6Rotz0YLnT5dKdo09JTuLCGcPa/bzhqZ5hBZntOt9pPU8YlMPvr5wBhI4HGNGvfdfrqKQk4dWbT2Z/ZUNCDkpKZJq6UaqL1NuB3j0y1ZlB0lkMxOGkc1LjMMWtiHC83ff9qhNHtTu/7Xww+PwmWF93nB2UG3nWzM4wfURBu7/RKA30SnUZZzpid4s+2x5tGr5AiDdCiz4WSXZkPueY9gdJZ3Tn9kO1wbJzjhnMl44ayI++PImkNmbBVN1PUzdKdRGnJ407Z+607sOnKvZGGDAVi+/PO5rbX1zFNFeeO1rjB+XyjRNGBfPzTr0eu+a4uNRNdT4N9Ep1keW7rEA/ekBT3/b01MiBvtEfIDlJ2pwzPlpThufzz+/M6fD591w4OS71UN1DUzdKdZE6r4/Jw/JCet04vWpqwhb89jQGEmLBCtU7RLOU4GMiclBE1rrKfiwie0XkC/tnnmvfnSKyVUQ2icg5nVVxpXqamgYf/bPTQ8qcoP/tZ1eGlJfXNVKYpYOCVHxE02R4Ajg3QvlvjDHT7Z9FACIyCWst2WPsc/7kLBauVF9X4/GREzbQxz14yT0H/eFaD/1zQj8UlOqoNgO9MeZDINoFvi8A/maM8RhjdgBbgdkx1E+pXqO6wUdu2CjT4YVZDMnPCO53H5uXqbfQVHzEkgT8toistlM7zhjrYcAe1zEldlkzIrJARJaJyLKysrIYqqFU4jPGUFHfSH6EdMytZ00ACC7ZB+1fCUqp1nQ00D8EjAWmA6XAr+3ySF0Emi+fAxhjHjHGzDLGzCoqSoxVWJTqLKPvXITXF4i41mueXfbwh9uCZR5foEPTCSsVSYfeScaYA8YYvzEmAPyFpvRMCTDCdehwYF9sVVSqZ3N3nSzIbD7T46xR1hfiZ5buBqCqoZEjtV5t0au46VCgF5EhrocXAU6PnNeBK0QkXURGA+OBz2KrolI928EqT3A7Uos+fNKxqT9+myO1Xm3Rq7hp826PiDwHzAUGiEgJ8CNgrohMx0rL7ARuADDGrBORF4D1gA+42Rjjj3RdpfqKqoam3HtBhBx9SxN0HarxRixXqr3aDPTGmCsjFD/ayvH3AvfGUimlepMq103WSC36lpRVe9o+SKko6HdDpTrZNtdkYANa6Bt/7cnFzRbT+J9zJnZqvVTfoYFeqU6w41AtxXcs5NPth9nlCvQtra/qLBLe6FpWsF9O1yzRp3o/HZGhVBx5fH6m/Ojt4DTDr67cGzKNb0urO6WlJNHoD4T00BlRmBXxWKXaSwO9UnF0qMYbsth3wBg83uZrwoZLTU4iYJpGx/7swslkR1irVamO0NSNUnHkCZtu+IVlJdR5reB9eys5d5/94fDZDmu2EV0TVcWTBnql4sg9X43jrXUHmD6igJtPH9fieSeOtRb1OFRj9bTJTNNAr+JHA71ScRQ+r7yjrbnlncDudMXUQK/iSQO9UnEUqUUPoeutRpKabN2wrbLP19SNiicN9ErFUUsteicl0xJnbVhnBsssbdGrONJAr1Qc1bimO2gPJ9C/snIvoC16FV8a6JWKI3eL/p4LjglunzJuQKvnOakbR4YGehVH2lFXqTi6/+3NAOy8bz4A500ZQkWdl+FtDH4Kv1mrqRsVTxrolepEA3LSW5zfxi0lLNBrrxsVT5q6USpO9lXUd/jc8KkRdNERFU8a6JWKk8se/gSAo4fktfvcnPQUhhVkBh+758dRKlYa6JWKk5Jyq0X/HyeM7ND5mq5RnUUDvVJxcvHMYQBcMH1Yh873B0w8q6NUkAZ6peIkLyOVvIwUcjo466QT6O+5cHI8q6VU24FeRB4TkYMistZV9isR2Sgiq0XkFREpsMuLRaReRL6wf/7cmZVXKpE0+gMtzjcfDSfQZ8RwDaUiieYd9QRwbljZYmCyMWYqsBm407VvmzFmuv1zY3yqqVTia/QHgiNcO8IJ9JqrV/HW5rvSGPMhcCSs7G1jjDME8FNgeCfUTakepdHZ2vC5AAAfS0lEQVRvYgv0xmnRa6BX8RWP74jfBN50PR4tIitF5AMRObWlk0RkgYgsE5FlZWVlcaiGUt3L6w80m8qgPRq81qIlORk6jlHFV0yBXkTuAnzAM3ZRKTDSGDMDuBV4VkQidio2xjxijJlljJlVVFQUSzWUSgiNvthSN6l2bn5ofmYbRyrVPh1+V4rI1cD5wNeNsb5zGmM8xpjD9vZyYBswIR4VVSrR1Xn9MeXX771wMl+ZNpRhhRroVXx16DuiiJwLfA84zRhT5yovAo4YY/wiMgYYD2yPS02VSnCHajyM6Nf65GWtOW/KEM6bMiSONVLKEk33yueAT4CJIlIiItcBfwBygcVh3SjnAKtFZBXwInCjMeZIxAsr1cscqfXSPzutu6uhVDNttuiNMVdGKH60hWNfAl6KtVJK9UQNjX6dR14lJB2ZoVSceP0B0nWwk0pA+q5UKg6MMXh9sY2MVaqz6LtSqTjwBQwB03ylKKUSgb4rlYoDry8ANF9ARKlEoO9KpeJAA71KZPquVCoOvH4N9Cpx6btSqTgItug1R68SkL4rlYoDj6ZuVALTd6VSceC06LUfvUpE+q5UKg40R68Smb4rlYqDphy9ToGgEo8GeqXasGznEYrvWMj9b21q8RjtXqkSmb4rVZ93sLqB4jsW8viSHRH3v7SiBIA/vLe1xWt4/dbqUBroVSLSd6Xq897faC1l+ZN/rKf4joXsOlwbsn9gbkab12ho1JuxKnHpu1L1eeGrQp32q/fZX9kQfFxZ3xjcPlLrjXgN55j8zNROqKFSsdFAr/q8SOu8ri+tDG67A/3MexZHvEZFnXVMQZYGepV4NNCrPs/j8zcrc/eeqaiL3Ip3q6j3kpacRKYuPKISUFSBXkQeE5GDIrLWVdZPRBaLyBb730K7XETkQRHZKiKrRWRmZ1VeqXio91qB/vvzjgqW1Xl9we0KV4u+JZV1jeRnpSIi8a+gUjGKtkX/BHBuWNkdwLvGmPHAu/ZjgPOwFgUfDywAHoq9mkp1nh2HaklLTuKqE4v561WzAKjzNrXy95bXt3mNirpGCjVtoxJUVIHeGPMhEL7I9wXAk/b2k8CFrvKnjOVToEBEdGl7lbDKqj0Myk8nIzWZmaMKgaabrnVeHwerPa2ef+/C9fxz3X5y0ttcglmpbhFLjn6QMaYUwP53oF0+DNjjOq7ELgshIgtEZJmILCsrK4uhGkrFxusPBG/IFmalkpaSxIGqBv697RAn3fcvAH54/qTg8caYkPP/8pHV/37F7oouqrFS7dMZN2MjJSlNswJjHjHGzDLGzCoqKuqEaigVnUZ/IDi9sIgwJD+D0soG7n9rU7A3zezR/bj9nIlA00yV4U4dP6BrKqxUO8US6A84KRn734N2eQkwwnXccGBfDM+jVKdq9JuQLpaD8zIorawnxVU2ekA2WXZ/e3f+3t26f/q647ugtkq1XyyB/nXganv7auA1V/lVdu+bE4BKJ8WjVCJq9AdITW76IlqYlUZlfWNIV8ns9BSy06wcvLtHTrWnaVupRBXV3SMReQ6YCwwQkRLgR8B9wAsich2wG7jUPnwRMA/YCtQB18a5zkrFldcXCGnRZ6UlU+f1U98Y2r/eGUFb7/Wz50gdWWnJVDdooFeJL6pAb4y5soVdZ0Q41gA3x1IppbqKMYaVuyuYPbpfsCwzLZl6rz+k5Q4EUze1Xj9n/eZD0pKTeOHGE7u0vkp1hI6MVX3aZzuO4PUH+HjroWBZdnoKtV5fSC4emlr0q/ZYvWu8/gBVUQymUqq7aaBXfVpDhB40manJNDQGgj1uhhVkAgRz9D96fV3w2KoGDfQq8WmgV32az14C8E9fb5qpIzvdarkfqfVyyczhvPvd04Cm1I1bVb2V3ln8/+Z0dlWV6jAN9KpPe/jD7QCMLcoJlmWmpbi2k8iwe99kRRj5WmaPmh1emNWZ1VQqJhroVY+3fl9VcCm/9vpshzWzh3vBkCxXt8o7zjs6uD2sIJPjXTdtAd5at5/UZCEjVf+UVOLSd6fq0fZW1DPvwY/42cL1MV0n3RWondQN0Gz+mqMG54Y83nOkjrwMnbVSJTYN9KrH8gcMVz26FIC31x1o9/k1rsFO4pq5Iyut5V7Hx4W16Ks9Pg63sOqUUolCA73qsf78wTa2lVnru+6vamh3+qba7jGTnCQMzE0Plke66eo4f+pQ5k/RyVhVz6KBXvVYG0qrQh7/7fPd7Tq/1m7R/+by6SQlNbXow9eQDXfl7JHteh6lupsGetVjhU+J+sPX1kU8riU1HmtAVHZYYE+2g/6EQTnNzgE4ZfwA1v/0nOAN3NvOntCu51Wqq2mgVz1Xs8mv28dp0WeH3XDNy7BWipoyrKDFc7PSUoLTFc+dOLDF45RKBLokjuqxFq5pmhS1ICsVf6B9kd8J9OE9a4YWZPLSTSdxzNC8qK4zKC+jXc+rVFfTFr3qkdzzwD/1zdlcOL3ZImZtcuayiXTz9dhRhcGBUm1xd8dUKhFpoFc9khOk7zjvKOZMKCIvI4Uaj6/ZMn+tqfVGTt20V0aKBnqV2DTQqx7JmQc+N8MK0jkZKRhDsxknW9NSjr693D12lEpEGuhVj7S3oh6AATlW//ecdOsGak07VnyqtXvdZEWZolGqp9JAr3qkHYesgVITB1lTEuTYLfv2rPhU5/WRlZbc4Rb53fOP5vSJurC9Snwa6FWP5Czzl2XfCHX6tL+7IfqpEGo8/lanO2jL9aeO4fFrZ3f4fKW6SocDvYhMFJEvXD9VIvIdEfmxiOx1lc+LZ4WVAvDYgd7pGeN0cXxz7f6or7H7SK32mFF9QoebM8aYTcB0ABFJBvYCr2AtBv4bY8z9camhUhE0OIHe7vEyfYQ1uOmUcQOiOv+N1ftYsvVw51ROqQQTr9TNGcA2Y8yuOF1PqVbVN/pJEkhNds1Rk5qM1x/dxGafbtcgr/qOeAX6K4DnXI+/LSKrReQxESmMdIKILBCRZSKyrKysLE7VUH1FQ2OAzNTkkHng01KSgimdtrinJVaqt4s50ItIGvAV4O920UPAWKy0Tinw60jnGWMeMcbMMsbMKirSngsqenuO1PHoxzuoDeszn56SxGur9vHhZqvh8P6mgyxcXRrpEsHBUoVZqZ1bWaUSQDxa9OcBK4wxBwCMMQeMMX5jTAD4C6DdElTcGGOY/+BHEfdV1jdSUdfIVY99BsA1j3/Ozc+uaHac1xfg5RV7Abh7/qTOq6xSCSIegf5KXGkbEXGvynARsDYOz6EUAB5fgKoW+sp7Wlh4pPiOhbz2hRXYqxsa+WhLU6rwkmOHx7+SSiWYmAK9iGQBZwEvu4p/KSJrRGQ1cDrw/2J5DqXcalsZ+eqebbIhLFf/4LtbAPjO377guieXdU7llEpQMU3yYYypA/qHlX0jphop1YLSynpO/Pm/Wtz/2s0nM/2ni8nPTKWkvD5kX2qy1aZZt69pVapnrj++cyqqVILRkbGqx7h34YaQx+ErQ6UkJ/GlowaSmiyU14Uu2L3lYA0AEwbnBsuOHRWxQ5hSvY4uPKJ6jDdcPWje+K9TGD0gu9kx6SlJeHwBfvHmxpByf8BQWd/IwaoGAPplp0U937xSPZ0GetUjTRycG0zHuBVmp1Fa2UBpZQPHFRfy+c7y4L5pP3kbgOtPGc1t50zssroq1d00daMS0qo9FSx4ahleuyfND14N7bwVKchDaDrmnGMGRzxm9uh+2ppXfYoGepVQqhsaeX/TQS744xLeXn+AtfsqAXj60+hm15g8LD+4/aWjIi/anZepg6RU36KpG5VQvvfSahataZqBsqHRH9JV8srZIxk3MKfF8/Mymt7SY4oiHxe+GLhSvZ226FVCKa1sCHm8fGc5f19eEnw8b8pgrjtldIvnZ4fNL3/3/KObHZOXoS161bdo00YlFPdUY4Py0tl9pI5fL94MwElj+3PS2NanIU5KEsYWZXPUYGvw1PWnjsEfMPzc1QtnSEFG3OutVCLTQK8SSkVdY3B7UF5GSAt/wZwxJEex7N87t54WMqtlYVYaANedMpp5Uwa3eCNXqd5KA72KiTEmJKjGwucPsP1QLRfPGMYPzp/Ez9/cwFvrrKUBi/tncdqE6GY5Da/PJccOJylJuHD6UFI0yKs+SN/1qsOW7TzC6DsXsflAdVyu9+xnuwH4cEsZhdlpDM7LoLLeauGfO3lIhz9QkpOErx47XIO86rP0na867KlPrC6P8Vqt6UitNW3BjJFWX3h3N8iv6iyTSnWYpm5Uh/kDBmgK0B31nb+t5MxJg4LX++3l0wHIcvWgGV6YGdNzKNWXaYtedVidvUrTb9/ZwoKnQqf+XV1SQfEdCxl950J8razjerCqgVe/2Me3n13J9rJaivtnkW33c58/tWlpAx3JqlTHaaBXHVbraRrI9Pb6A8Ftf8DwlT8sAcAYGHfXm2y1Z48M94kr7bOtrIZBeU1dH/MzU9n8s/NY8YOz4l11pfoUDfSqw6pbWARkz5G6ZmVPf7KzWZk/YPjR6+uCjzfur2ZATnrIMWkpSfTLToupnkr1dRroVYfVeBpJT2l6CzmrP+0pbwr0L954IgDDC7Oanb9uX2VIv3mA/jka1JWKt5gDvYjstJcO/EJEltll/URksYhssf/VFR56oYraRq6cPZIZIwsA2F5WC8Cm/VZ3y1vOGM/MkYUkCVQ1NDY7P3y6AyA4W6VSKn7i1aI/3Rgz3Rgzy358B/CuMWY88K79WPUiXl+Aao+P/tlpPHCZ1Uvmy3/4mFV7Kvhgs7X49nfOHE9SkhAwsNiVw3fc8PTyZmWRFhNRSsWms1I3FwBP2ttPAhd20vOoblJhL9VXmJ3GqH5NaZlFa0r5aMshIHSE6sb9oYOqyl1dMmfZc8h//L3Tuf7UMZ1WZ6X6qngEegO8LSLLRWSBXTbIGFMKYP8beWJw1WMdtgN1/+w0kpKEo+y1WN05+3DugVVnPvABAKeMG8Dj1x7HO7eexvDCrKjmslFKtU88Av3JxpiZwHnAzSIyJ5qTRGSBiCwTkWVlZWVxqIbqSs60B06PmOdvsG66ltc1z8Xfbi/bd8Ujn2KMYdWeiuAHxXHF/cjNSG11jnmlVGxiDvTGmH32vweBV4DZwAERGQJg/3swwnmPGGNmGWNmFRVFN1mVShzODVdnuoL8zFQG5KRHHCXrzrvXeHxc8Mclwcej+jfvjaOUiq+YAr2IZItIrrMNnA2sBV4HrrYPuxp4LZbnUYmnvM5LUW46aa5UTUZqEgeqrJ40V584Klg+bURB03m1oS3+MUV681WpzhbrXDeDgFfsm24pwLPGmH+KyOfACyJyHbAbuDTG51EJpqzaQ7+s0D7vGanJLNtVDsDciU23ZYbmN4123X2kjiH51jzzZx49iKnDC1BKda6YAr0xZjswLUL5YeCMWK6tEtvmAzVMGZ4fUpaR2tS6H+wK7iLC1SeO4slPdvHUJzvJy0jlqMG5/PXqWSilOp+OjFVBWw/WBPvAt8YYw/7KBkaEjXZNs+d7nzIsP9gLx3H3+ZMAa06cTQeq6ZcdOtWBUqrz6DTFKsjp8rjzvvmtHldR14jXH6AoNzRYVzVYUyB8/fiRzRYJCV++L06LUimloqAtegVY8844nFWdIqn1+Ph85xEABoYF+oA9n7w7beP26Z1N2bx9FfUdrqtSqn000CvKa73Mf/Dj4OMVu8tbPPaWv33BAnvqgvBAP9FO1wzMjRzoB+dncNksa6WoB6+cEVOdlVLR09SNarbm67aDNZw+MfJg5k+2HQpuTxgUmoe//9JpnDt5MEcPyQ0/LehnF07hO2dOaDYdsVKq82iLXnGoxhrkNGdCEYPzMnhnQ/MJyBy13qbFRgrD5onPTk/hgunDWl3EOy0liaEFuiygUl1JA73iSK0HgPsvncrkYXl8uv0Ih2usMq8vwPVPLuPf2w7R0NgU5J1pDZRSiU9TN4rPd5aTm5FC/+x05kwo4p0NB/nDe1vZerCGq08s5p0NB9hQWsUwuyX+i0umcPlxI7u51kqpaGmg72OWbj9MdnoKk4dZg50CAcM7Gw5w/tQhJCcJM0ZYc9c8vmQnAMMLreC+t6KevXZPmcH5mnpRqifRQN/HXP7Ip0BTX/mD1R7qvP7gVARjB4bOPfPcZ3uaXSM/M7WTa6mUiifN0fchxphm22XVVi7e6SqZlZbC78O6Pg7MTQ8G9+L+WUwdFjr1gVIqsWmg7yM8Pj8r91QEH0/9ydt4fH7e32TNIO0e5Tp/yhDOnjQo+Pjqk4q5/LgRAPzvxVNI0sVBlOpRNHXTR/zqn5v468c7go+rG3x8uPkQv168GQgN9ElJwiNXzWLe7z5ifWkV155cjDHWkn8njunf5XVXSsVGA30f4UxbAHD9KaP568c7eG9T03owkQYw/f3GE2n0B8hKs94mZx8zuPMrqpSKO03d9AHltV5WlVSSkiQs+u9TWTDHWoD7hc+tG603zBlDRmpys/Oy01MoCJtzXinV82ig78WcG65//nAbAL6AYdLQPAbmZXD86H747EnIbpo7ttvqqJTqfJq66aXqvD4m/fAt5k8dwuL11pQGk4bkBfcnuaYp0O6SSvVuGuh7qU+2HQZg4erSYNnT180ObmelNaVqWpubRinV83U4dSMiI0TkPRHZICLrROQWu/zHIrJXRL6wf+bFr7oqWj9buCHk8fK7z6S/64br/ZdOY86EIq4/ZXRXV00p1cViadH7gO8aY1aISC6wXEQW2/t+Y4y5P/bqqY7YXlbDjkO1IWX9w3rVFGan8dQ3Z6OU6v063KI3xpQaY1bY29XABmBYvCqmOm7rwRoAnrj2uG6uiVIqEcSl142IFAMzgKV20bdFZLWIPCYihS2cs0BElonIsrKythekVtFzVoCaOryAcQNzdEphpfq4mAO9iOQALwHfMcZUAQ8BY4HpQCnw60jnGWMeMcbMMsbMKioqirUaylZa2bQWa7/sNN659TRuPn1cN9ZIKdXdYgr0IpKKFeSfMca8DGCMOWCM8RtjAsBfAE0Ed6EfvrYOgBdvPLGba6KUShSx9LoR4FFggzHmAVf5ENdhFwFrO169jqn1+PD4/G0fGMYfMG0f1EkCAUMgyuc3xuAPGP65thSvLxAs31/ZwOL1B7jljPHMKu7XWVVVSvUwsfS6ORn4BrBGRL6wy74PXCki0wED7ARuiKmG7eAPGJ5ZuosfvraO5CRh+d1nRj2E/1tPLWPx+gMMyc/gjvOO4oLpXXdfubzWy4x7FvOD8ydxXRvdHasaGjnvtx8FFwG57+IpXDHbWu1pw/4qAE4cqxOPKaWadDjQG2M+BiKNtFnU8ep0nNcX4PElO/j5mxsBK+if+cAH/OKSqSxas5+rTxpFdYOPIfkZ5GemhnQ3bPQHgqNHSysbuOVvXzB5WD5ji3LiWj8RSE1u/iXq0+3W4KY/vbe1zUD/6Ec7gkEe4I6X1/ClowcyMDeDtSWViMAxQ/NauYJSqq/pNSNjL35oCWv3VoWUHarxct2TywB4aUVJyL7rThnNDXPGsKqkkheXW5N7/c+5E9lbXs8zS3fzwaaydgX6dfsqeeDtzQzISefL04ZyyvgB7K9s4KwHPkAE8jJTGZSXwfMLTiDFDva1Hh/ldV5uemYFAIdrvby5ppRjRxUyMC8DgIZGP4dqPKSnJFOUm84Hm8s4ZmgeVxw3gmqPj1/+cxNP/XsXt50zkVUllYwZkE1uhk5poJRq0isCfUWdNxjk7790GudNHsz7m8q4+dkVLZ7z6Mc7eOLfO0Py8gtOHUNKchKvr9rHrsO1LZ7rVuPxcfy971Drbbon8Pwy64PjB+dPotrjA6CqwUdJeT3j7nqTHT+fx54j9cz51XvNrucE/VH9s7hs1gh+9damZsfcdvYEvnFiMWBNdfDm2lJmjirgnQ0HuPTY4VHVWynVd/SKQP+/i6zh/s9+63hOGjsAgHlTmuZOv+fCyYwrygnmrv0Bw5xfvheSArn25OJgS3tgbjobSqujeu5rHvssJMi73fPG+ojlG0qree6z3SFlP794CkPyM1i64wjPLt3NrsN1EYM8wOXHjQxuTx9RwEdbDvHNJ6xvLlcePzLiOUqpvqvHB/oXl5fwwrISrpw9MhjkwZqo69eXTuO7f1/F/ClD6JfddFM2OUn409dn8vCH21i0Zj8AxwxtWgd1zoQiHl+yk037q5k4OBewerRkpiUHZ3pcvusIN/3fCg7aa67eNHcst509EY/Pz5YDNVzwxyXB6+28bz4en581JZV89c+fMO/Bj4L71vz47JBUy9yJA7niuBGc9qv3AXjjv05h8rB8ajw+7n5lDceOKgxZDWqKa/3W//rSOGaOjDg+TSnVh4l7wejuMmvWLLNs2bIOnXvxn5awYncFK39wFoXZ7V8k40v3v8/2Q7V8eucZDM638uIHqxqY/b/vAnDZrOFsOVjDyt3WequnTSji5HH9uf+tzXj9VtfGD26fy6j+2SHX3XKgmh+8tpZLZg7n0lnWequ1Hh/H/Oit4DHZacms++m5zerk8we46rHPGJKfya8vm9Zq/Y0x/OQf6zltQhGnHzWw3b+/UqrnEpHlxphZbR7XkwP9yt3lXPSnf3Pz6WO5/ZyjOvTcOw7V8sWeci6aEZrbvuVvK3nti32tnvv4NccxZ0IRye1YLPvHr69j6Y4jGGO496IpHDtKW+BKqY7pE4G+uqGRv3y4nQWnjSUnPf5ZqMr6Rl7/Yi8j+2czc2QBn2w7jMcX4OMth/jGiaOY7EqbKKVUV+sTgV4ppfqyaAO9rhmrlFK9nAZ6pZTq5TTQK6VUL6eBXimlejkN9Eop1ctpoFdKqV5OA71SSvVyGuiVUqqXS4gBUyJSBuyK4RIDgENxqk5n0TrGh9YxPnpCHaFn1LM76zjKGFPU1kEJEehjJSLLohkd1p20jvGhdYyPnlBH6Bn17Al11NSNUkr1chrolVKql+stgf6R7q5AFLSO8aF1jI+eUEfoGfVM+Dr2ihy9UkqplvWWFr1SSqkWaKBXSqlerkcHehE5V0Q2ichWEbmjG+sxQkTeE5ENIrJORG6xy/uJyGIR2WL/W2iXi4g8aNd7tYjM7MK6JovIShF5w348WkSW2nV8XkTS7PJ0+/FWe39xF9WvQEReFJGN9ut5YoK+jv/P/r9eKyLPiUhGd7+WIvKYiBwUkbWusna/diJytX38FhG5ugvq+Cv7/3u1iLwiIgWufXfaddwkIue4yjvtbz9SHV37bhMRIyID7Mfd8jq2mzGmR/4AycA2YAyQBqwCJnVTXYYAM+3tXGAzMAn4JXCHXX4H8At7ex7wJiDACcDSLqzrrcCzwBv24xeAK+ztPwM32dv/CfzZ3r4CeL6L6vckcL29nQYUJNrrCAwDdgCZrtfwmu5+LYE5wExgrausXa8d0A/Ybv9baG8XdnIdzwZS7O1fuOo4yf67TgdG23/vyZ39tx+pjnb5COAtrMGdA7rzdWz379RdTxyH/4wTgbdcj+8E7uzuetl1eQ04C9gEDLHLhgCb7O2HgStdxweP6+R6DQfeBb4EvGG/OQ+5/siCr6n9hj7R3k6xj5NOrl+eHUAlrDzRXsdhwB77jzjFfi3PSYTXEigOC6Lteu2AK4GHXeUhx3VGHcP2XQQ8Y2+H/E07r2NX/O1HqiPwIjAN2ElToO+217E9Pz05deP8sTlK7LJuZX8tnwEsBQYZY0oB7H8H2od1V91/C/wPELAf9wcqjDG+CPUI1tHeX2kf35nGAGXA43Z66a8ikk2CvY7GmL3A/cBuoBTrtVlOYr2Wjva+dt39d/VNrBYyrdSly+soIl8B9hpjVoXtSpg6tqYnB3qJUNatfUVFJAd4CfiOMaaqtUMjlHVq3UXkfOCgMWZ5lPXojtc3Besr80PGmBlALVa6oSXd8h6w89wXYKUThgLZwHmt1CXh3qu0XKduq6uI3AX4gGecohbq0qV1FJEs4C7gh5F2t1CXhPo/78mBvgQrZ+YYDuzrprogIqlYQf4ZY8zLdvEBERli7x8CHLTLu6PuJwNfEZGdwN+w0je/BQpEJCVCPYJ1tPfnA0c6uY4lQIkxZqn9+EWswJ9IryPAmcAOY0yZMaYReBk4icR6LR3tfe265TW1b1aeD3zd2LmOBKrjWKwP9VX2389wYIWIDE6gOraqJwf6z4Hxdk+HNKybXK93R0VERIBHgQ3GmAdcu14HnLvtV2Pl7p3yq+w79icAlc7X685ijLnTGDPcGFOM9Vr9yxjzdeA94Kst1NGp+1ft4zu1RWKM2Q/sEZGJdtEZwHoS6HW07QZOEJEs+//eqWfCvJYu7X3t3gLOFpFC+5vL2XZZpxGRc4HvAV8xxtSF1f0Ku9fSaGA88Bld/LdvjFljjBlojCm2/35KsDpf7CeBXsdWddfNgTjdMJmH1cNlG3BXN9bjFKyvZauBL+yfeVh52HeBLfa//ezjBfijXe81wKwuru9cmnrdjMH649kK/B1It8sz7Mdb7f1juqhu04Fl9mv5KlaPhYR7HYGfABuBtcDTWD1DuvW1BJ7DumfQiBWMruvIa4eVJ99q/1zbBXXcipXPdv52/uw6/i67jpuA81zlnfa3H6mOYft30nQztltex/b+6BQISinVy/Xk1I1SSqkoaKBXSqleTgO9Ukr1chrolVKql9NAr5RSvZwGeqWU6uU00CulVC/3/wET5Ks+xaXUjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(smooth(episode_durations, 20))\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n",
    "loss, v_loss, a_loss = zip(*step_losses)\n",
    "\n",
    "plt.plot(smooth(v_loss, 100))\n",
    "plt.title('Value loss')\n",
    "plt.show()\n",
    "plt.plot(smooth(a_loss, 100))\n",
    "plt.title('Actor loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de8c4cba2ebd1a8bba2236f92a0b550c",
     "grade": false,
     "grade_id": "cell-8d15d4c9c0310bec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What is the difficulty of training AC algorithms? What could you try to do to overcome these difficulties? Hint: look at some online implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1e51e82a7730101dfd07b2f0e470d1b4",
     "grade": true,
     "grade_id": "cell-f68c6134a9df40b9",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# TODO: Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5947c1e643f533003715ae8da659af9e",
     "grade": false,
     "grade_id": "cell-ad1138b69e6728a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Deep Reinforcement Learning (5 bonus points)\n",
    "Note that so far we used the state variables as input. However, the true power of Deep Learning is that we can directly learn from raw inputs, e.g. we can learn to balance the cart pole *by just looking at the screen*. This probably means that you need a deep(er) (convolutional) network, as well as tweaking some parameters, running for more iterations (perhaps on GPU) and do other tricks to stabilize learning. Can you get this to work? This will earn you bonus points!\n",
    "\n",
    "Hints:\n",
    "* You may want to use [Google Colab](https://colab.research.google.com/) such that you can benefit from GPU acceleration.\n",
    "* Even if you don't use Colab, save the weights of your final model and load it in the code here (see example below). Hand in the model file with the .ipynb in a .zip. We likely won't be able to run your training code during grading!\n",
    "* To run the code below, you need to install `torchvision`, for this uncomment the two lines in the cell below or run the command in a terminal. Note: you may need to restart the terminal after installing.\n",
    "* Preprocessing is already done for you, and the observation is the difference between two consequtive frames such that the model can 'see' (angular) speed from a single image. Now do you see why we (sometimes) use the word observation (and not state)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# conda install torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f660e1484fe2bf60d66467326eacb1ba",
     "grade": false,
     "grade_id": "cell-9c9dfa80827c5680",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "class CartPoleRawEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._env = gym.make('CartPole-v0', *args, **kwargs)  #.unwrapped\n",
    "        self.action_space = self._env.action_space\n",
    "        screen_height, screen_width = 40, 80  # TODO\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        return self._env.seed(seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        self.prev_screen = self.screen = self.get_screen()\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        s, r, done, info = self._env.step(action)\n",
    "        self.prev_screen = self.screen\n",
    "        self.screen = self.get_screen()\n",
    "        return self._get_observation(), r, done, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return self.screen - self.prev_screen\n",
    "    \n",
    "    def _get_cart_location(self, screen_width):\n",
    "        _env = self._env.unwrapped\n",
    "        world_width = _env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(_env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self._env.unwrapped.render(mode='rgb_array').transpose(\n",
    "            (2, 0, 1))  # transpose into torch order (CHW)\n",
    "        # Strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, screen_height * 4 // 10:screen_height * 8 // 10]\n",
    "        view_width = screen_height * 8 // 10\n",
    "        cart_location = self._get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescare, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        #return screen.unsqueeze(0).to(device)\n",
    "        return resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        return self._env.close()\n",
    "\n",
    "raw_env = CartPoleRawEnv()\n",
    "s = raw_env.reset()\n",
    "\n",
    "# \n",
    "s, r, done, _ = raw_env.step(env.action_space.sample())\n",
    "\n",
    "raw_env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(raw_env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "\n",
    "# Observations are (-1, 1) while we need to plot (0, 1) so show (rgb + 1) / 2\n",
    "plt.figure()\n",
    "plt.imshow((s.cpu().squeeze(0).permute(1, 2, 0).numpy() + 1) / 2,\n",
    "           interpolation='none')\n",
    "plt.title('Example observation')\n",
    "plt.show()\n",
    "raw_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30, finished 12 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.75\n",
      "\t\t\tmax episode duration: 30\n",
      "\t\tAvg Loss. Total: 0.5699072301387786; Actor: 0.465696515639623; Critic: 0.3370589723189672\n",
      "Step 60, finished 25 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.76\n",
      "\t\t\tmax episode duration: 57\n",
      "\t\tAvg Loss. Total: 0.800053068002065; Actor: 0.6563565810521443; Critic: 0.4718747764825821\n",
      "Step 90, finished 47 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.933333333333334\n",
      "\t\t\tmax episode duration: 76\n",
      "\t\tAvg Loss. Total: 0.7507122536500295; Actor: 0.618131572008133; Critic: 0.4416464736064275\n",
      "Step 120, finished 65 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.866666666666667\n",
      "\t\t\tmax episode duration: 76\n",
      "\t\tAvg Loss. Total: 0.7708434462547302; Actor: 0.6329330106576284; Critic: 0.45437694489955904\n",
      "Step 150, finished 80 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.566666666666666\n",
      "\t\t\tmax episode duration: 52\n",
      "\t\tAvg Loss. Total: 0.7689405858516694; Actor: 0.6264227529366811; Critic: 0.45572921335697175\n",
      "Step 180, finished 95 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.2\n",
      "\t\t\tmax episode duration: 54\n",
      "\t\tAvg Loss. Total: 0.7761146008968354; Actor: 0.6313503891229629; Critic: 0.46043940683205925\n",
      "Step 210, finished 112 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.033333333333335\n",
      "\t\t\tmax episode duration: 54\n",
      "\t\tAvg Loss. Total: 0.7533176104227702; Actor: 0.620527050892512; Critic: 0.4430540810028712\n",
      "Step 240, finished 126 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.766666666666666\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.771054458618164; Actor: 0.6269398927688599; Critic: 0.45758451024691266\n",
      "Step 270, finished 146 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.0\n",
      "\t\t\tmax episode duration: 71\n",
      "\t\tAvg Loss. Total: 0.7608295818169911; Actor: 0.62297669450442; Critic: 0.44934123059113823\n",
      "Step 300, finished 161 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.766666666666666\n",
      "\t\t\tmax episode duration: 71\n",
      "\t\tAvg Loss. Total: 0.775279313325882; Actor: 0.6343970815340678; Critic: 0.45808077454566953\n",
      "Step 330, finished 171 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.4\n",
      "\t\t\tmax episode duration: 44\n",
      "\t\tAvg Loss. Total: 0.7843509256839752; Actor: 0.641151773929596; Critic: 0.46377504368623096\n",
      "Step 360, finished 187 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 26.5\n",
      "\t\t\tmax episode duration: 68\n",
      "\t\tAvg Loss. Total: 0.7715603212515513; Actor: 0.6334770341714223; Critic: 0.45482180217901863\n",
      "Step 390, finished 202 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 26.066666666666666\n",
      "\t\t\tmax episode duration: 68\n",
      "\t\tAvg Loss. Total: 0.7892301122347514; Actor: 0.643779742717743; Critic: 0.4673402428627014\n",
      "Step 420, finished 215 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.666666666666668\n",
      "\t\t\tmax episode duration: 68\n",
      "\t\tAvg Loss. Total: 0.7710846165815989; Actor: 0.6313084840774537; Critic: 0.45543037752310433\n",
      "Step 450, finished 230 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 25.266666666666666\n",
      "\t\t\tmax episode duration: 68\n",
      "\t\tAvg Loss. Total: 0.7655368407567342; Actor: 0.6284144997596741; Critic: 0.45132958590984346\n",
      "Step 480, finished 250 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.166666666666668\n",
      "\t\t\tmax episode duration: 61\n",
      "\t\tAvg Loss. Total: 0.7388531466325124; Actor: 0.6068628430366516; Critic: 0.4354217251141866\n",
      "Step 510, finished 265 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.9\n",
      "\t\t\tmax episode duration: 52\n",
      "\t\tAvg Loss. Total: 0.7394897143046061; Actor: 0.605465163787206; Critic: 0.4367571294307709\n",
      "Step 540, finished 280 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.966666666666665\n",
      "\t\t\tmax episode duration: 52\n",
      "\t\tAvg Loss. Total: 0.7393050352732341; Actor: 0.6076823314030965; Critic: 0.43546386460463204\n",
      "Step 570, finished 297 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.833333333333332\n",
      "\t\t\tmax episode duration: 74\n",
      "\t\tAvg Loss. Total: 0.7043821632862091; Actor: 0.5742193957169851; Critic: 0.4172724634408951\n",
      "Step 600, finished 314 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.8\n",
      "\t\t\tmax episode duration: 74\n",
      "\t\tAvg Loss. Total: 0.7570703665415446; Actor: 0.6167385955651601; Critic: 0.4487010627985001\n",
      "Step 630, finished 329 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.266666666666666\n",
      "\t\t\tmax episode duration: 45\n",
      "\t\tAvg Loss. Total: 0.7313726822535197; Actor: 0.6007511218388876; Critic: 0.4309971203406652\n",
      "Step 660, finished 345 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.8\n",
      "\t\t\tmax episode duration: 48\n",
      "\t\tAvg Loss. Total: 0.7074077169100443; Actor: 0.5741412699222564; Critic: 0.4203370819489161\n",
      "Step 690, finished 364 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.666666666666668\n",
      "\t\t\tmax episode duration: 41\n",
      "\t\tAvg Loss. Total: 0.7239294906457265; Actor: 0.5926239689191183; Critic: 0.42761750320593517\n",
      "Step 720, finished 380 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.766666666666666\n",
      "\t\t\tmax episode duration: 41\n",
      "\t\tAvg Loss. Total: 0.7312284847100575; Actor: 0.5906941095987955; Critic: 0.43588143587112427\n",
      "Step 750, finished 396 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.166666666666668\n",
      "\t\t\tmax episode duration: 71\n",
      "\t\tAvg Loss. Total: 0.7287286758422852; Actor: 0.591816716392835; Critic: 0.4328203211228053\n",
      "Step 780, finished 415 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.233333333333334\n",
      "\t\t\tmax episode duration: 71\n",
      "\t\tAvg Loss. Total: 0.6971737921237946; Actor: 0.5765426476796468; Critic: 0.4089024643103282\n",
      "Step 810, finished 435 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 16.933333333333334\n",
      "\t\t\tmax episode duration: 33\n",
      "\t\tAvg Loss. Total: 0.6879732052485148; Actor: 0.5646848599116008; Critic: 0.40563077926635743\n",
      "Step 840, finished 450 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.033333333333335\n",
      "\t\t\tmax episode duration: 66\n",
      "\t\tAvg Loss. Total: 0.6850522975126903; Actor: 0.5532845079898834; Critic: 0.40841004451115925\n",
      "Step 870, finished 467 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.333333333333332\n",
      "\t\t\tmax episode duration: 66\n",
      "\t\tAvg Loss. Total: 0.674217015504837; Actor: 0.5504245032866796; Critic: 0.3990047653516134\n",
      "Step 900, finished 486 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.233333333333334\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.6718405485153198; Actor: 0.5535874436299006; Critic: 0.39504682819048564\n",
      "Step 930, finished 501 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.433333333333334\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.6670848449071248; Actor: 0.5450518320004145; Critic: 0.3945589244365692\n",
      "Step 960, finished 516 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.933333333333334\n",
      "\t\t\tmax episode duration: 43\n",
      "\t\tAvg Loss. Total: 0.7152721087137858; Actor: 0.580630632241567; Critic: 0.4249567965666453\n",
      "Step 990, finished 535 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.233333333333334\n",
      "\t\t\tmax episode duration: 65\n",
      "\t\tAvg Loss. Total: 0.6875721037387847; Actor: 0.5679596265157064; Critic: 0.4035922904809316\n",
      "Step 1020, finished 549 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.7\n",
      "\t\t\tmax episode duration: 53\n",
      "\t\tAvg Loss. Total: 0.7286566376686097; Actor: 0.5842434495687485; Critic: 0.4365349173545837\n",
      "Step 1050, finished 565 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.0\n",
      "\t\t\tmax episode duration: 61\n",
      "\t\tAvg Loss. Total: 0.7087231794993083; Actor: 0.5843794047832489; Critic: 0.4165334721406301\n",
      "Step 1080, finished 583 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.933333333333334\n",
      "\t\t\tmax episode duration: 79\n",
      "\t\tAvg Loss. Total: 0.6823984901110332; Actor: 0.5562558561563492; Critic: 0.4042705655097961\n",
      "Step 1110, finished 599 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.433333333333334\n",
      "\t\t\tmax episode duration: 79\n",
      "\t\tAvg Loss. Total: 0.6870220839977265; Actor: 0.5595936924219131; Critic: 0.40722524126370746\n",
      "Step 1140, finished 615 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.633333333333333\n",
      "\t\t\tmax episode duration: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tAvg Loss. Total: 0.6951808234055837; Actor: 0.5655419399340947; Critic: 0.41240985492865245\n",
      "Step 1170, finished 632 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.8\n",
      "\t\t\tmax episode duration: 48\n",
      "\t\tAvg Loss. Total: 0.63165682554245; Actor: 0.5150492618481318; Critic: 0.3741321990887324\n",
      "Step 1200, finished 647 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.733333333333334\n",
      "\t\t\tmax episode duration: 58\n",
      "\t\tAvg Loss. Total: 0.6759719669818878; Actor: 0.5396684626738231; Critic: 0.4061377376317978\n",
      "Step 1230, finished 665 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.666666666666668\n",
      "\t\t\tmax episode duration: 58\n",
      "\t\tAvg Loss. Total: 0.6716877321402231; Actor: 0.5461045036713282; Critic: 0.3986354817946752\n",
      "Step 1260, finished 679 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.3\n",
      "\t\t\tmax episode duration: 53\n",
      "\t\tAvg Loss. Total: 0.6388321399688721; Actor: 0.5239425639311472; Critic: 0.37686085104942324\n",
      "Step 1290, finished 694 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.1\n",
      "\t\t\tmax episode duration: 43\n",
      "\t\tAvg Loss. Total: 0.7100765029589335; Actor: 0.5672686854998271; Critic: 0.426442156235377\n",
      "Step 1320, finished 714 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.833333333333332\n",
      "\t\t\tmax episode duration: 63\n",
      "\t\tAvg Loss. Total: 0.6553641398747762; Actor: 0.5382424016793569; Critic: 0.38624294598897296\n",
      "Step 1350, finished 725 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.066666666666666\n",
      "\t\t\tmax episode duration: 61\n",
      "\t\tAvg Loss. Total: 0.6724652628103892; Actor: 0.5371308734019598; Critic: 0.4038998275995255\n",
      "Step 1380, finished 736 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.566666666666666\n",
      "\t\t\tmax episode duration: 47\n",
      "\t\tAvg Loss. Total: 0.7275495330492655; Actor: 0.597984621922175; Critic: 0.4285572280486425\n",
      "Step 1410, finished 750 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 25.9\n",
      "\t\t\tmax episode duration: 79\n",
      "\t\tAvg Loss. Total: 0.692800122499466; Actor: 0.5659579267104466; Critic: 0.40982115467389424\n",
      "Step 1440, finished 764 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 27.333333333333332\n",
      "\t\t\tmax episode duration: 79\n",
      "\t\tAvg Loss. Total: 0.7344269076983134; Actor: 0.5858953018983205; Critic: 0.44147926370302837\n",
      "Step 1470, finished 777 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 28.8\n",
      "\t\t\tmax episode duration: 77\n",
      "\t\tAvg Loss. Total: 0.7387891014417013; Actor: 0.6090208162864049; Critic: 0.4342786967754364\n",
      "Step 1500, finished 793 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 26.7\n",
      "\t\t\tmax episode duration: 68\n",
      "\t\tAvg Loss. Total: 0.6902645587921142; Actor: 0.5499350885550182; Critic: 0.415297011534373\n",
      "Step 1530, finished 805 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 25.733333333333334\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.6926521996657053; Actor: 0.5628663718700408; Critic: 0.41121901174386344\n",
      "Step 1560, finished 826 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.133333333333333\n",
      "\t\t\tmax episode duration: 48\n",
      "\t\tAvg Loss. Total: 0.679411401351293; Actor: 0.5594087858994802; Critic: 0.39970700442790985\n",
      "Step 1590, finished 842 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.2\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.6233790477116903; Actor: 0.5080619106690089; Critic: 0.36934809188048046\n",
      "Step 1620, finished 860 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.366666666666667\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.6560149967670441; Actor: 0.5425335784753164; Critic: 0.3847482035557429\n",
      "Step 1650, finished 878 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.866666666666667\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.6195402006308238; Actor: 0.5005467742681503; Critic: 0.36926681250333787\n",
      "Step 1680, finished 896 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.333333333333332\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.6345089773337046; Actor: 0.5291905452807745; Critic: 0.3699137051900228\n",
      "Step 1710, finished 908 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.566666666666666\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.6527593672275543; Actor: 0.5299382885297139; Critic: 0.38779022097587584\n",
      "Step 1740, finished 924 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 25.566666666666666\n",
      "\t\t\tmax episode duration: 60\n",
      "\t\tAvg Loss. Total: 0.6732554137706757; Actor: 0.5488524774710337; Critic: 0.3988291730483373\n",
      "Step 1770, finished 941 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.9\n",
      "\t\t\tmax episode duration: 60\n",
      "\t\tAvg Loss. Total: 0.6923675835132599; Actor: 0.5555598785479864; Critic: 0.41458764175573987\n",
      "Step 1800, finished 957 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.166666666666668\n",
      "\t\t\tmax episode duration: 45\n",
      "\t\tAvg Loss. Total: 0.7223670522371928; Actor: 0.5827419817447662; Critic: 0.4309960603713989\n",
      "Step 1830, finished 972 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.3\n",
      "\t\t\tmax episode duration: 43\n",
      "\t\tAvg Loss. Total: 0.6429346621036529; Actor: 0.5353136877218883; Critic: 0.37527781625588735\n",
      "Step 1860, finished 988 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.5\n",
      "\t\t\tmax episode duration: 43\n",
      "\t\tAvg Loss. Total: 0.6522077818711599; Actor: 0.5418081204096477; Critic: 0.38130372365315757\n",
      "Step 1890, finished 1002 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.3\n",
      "\t\t\tmax episode duration: 54\n",
      "\t\tAvg Loss. Total: 0.7004635353883107; Actor: 0.5716997901598613; Critic: 0.4146136462688446\n",
      "Step 1920, finished 1016 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.766666666666666\n",
      "\t\t\tmax episode duration: 54\n",
      "\t\tAvg Loss. Total: 0.6958122372627258; Actor: 0.5632872611284256; Critic: 0.4141686032215754\n",
      "Step 1950, finished 1032 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.633333333333333\n",
      "\t\t\tmax episode duration: 52\n",
      "\t\tAvg Loss. Total: 0.685944926738739; Actor: 0.557516673207283; Critic: 0.4071865876515706\n",
      "Step 1980, finished 1049 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.4\n",
      "\t\t\tmax episode duration: 51\n",
      "\t\tAvg Loss. Total: 0.6696171899636586; Actor: 0.5526971906423569; Critic: 0.39326859414577486\n",
      "Step 2010, finished 1065 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.4\n",
      "\t\t\tmax episode duration: 41\n",
      "\t\tAvg Loss. Total: 0.6018465618292491; Actor: 0.4992040942112605; Critic: 0.3522445162137349\n",
      "Step 2040, finished 1084 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.766666666666666\n",
      "\t\t\tmax episode duration: 65\n",
      "\t\tAvg Loss. Total: 0.6680592825015386; Actor: 0.5487591842810313; Critic: 0.3936796878774961\n",
      "Step 2070, finished 1099 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.866666666666667\n",
      "\t\t\tmax episode duration: 65\n",
      "\t\tAvg Loss. Total: 0.6736136575539907; Actor: 0.5444984525442124; Critic: 0.4013644297917684\n",
      "Step 2100, finished 1112 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.133333333333333\n",
      "\t\t\tmax episode duration: 65\n",
      "\t\tAvg Loss. Total: 0.6840929130713145; Actor: 0.5723461518685024; Critic: 0.39791984160741173\n",
      "Step 2130, finished 1132 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.6\n",
      "\t\t\tmax episode duration: 45\n",
      "\t\tAvg Loss. Total: 0.656006666024526; Actor: 0.5227394819259643; Critic: 0.39463692605495454\n",
      "Step 2160, finished 1148 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.5\n",
      "\t\t\tmax episode duration: 45\n",
      "\t\tAvg Loss. Total: 0.6113847315311431; Actor: 0.5018123547236125; Critic: 0.36047855218251545\n",
      "Step 2190, finished 1167 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.466666666666665\n",
      "\t\t\tmax episode duration: 47\n",
      "\t\tAvg Loss. Total: 0.650919775168101; Actor: 0.5284997582435608; Critic: 0.3866699030001958\n",
      "Step 2220, finished 1180 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.733333333333334\n",
      "\t\t\tmax episode duration: 47\n",
      "\t\tAvg Loss. Total: 0.6777638514836629; Actor: 0.5558790177106857; Critic: 0.39982433716456095\n",
      "Step 2250, finished 1195 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.7\n",
      "\t\t\tmax episode duration: 54\n",
      "\t\tAvg Loss. Total: 0.6753545900185903; Actor: 0.5459653774897257; Critic: 0.4023718992869059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2280, finished 1218 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.233333333333334\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.6341707626978557; Actor: 0.5241309245427449; Critic: 0.37210529247919716\n",
      "Step 2310, finished 1234 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.133333333333333\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.6438663810491562; Actor: 0.5280677556991578; Critic: 0.3798325071732203\n",
      "Step 2340, finished 1247 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.733333333333334\n",
      "\t\t\tmax episode duration: 49\n",
      "\t\tAvg Loss. Total: 0.6267407655715942; Actor: 0.5121858855088551; Critic: 0.37064782281716663\n",
      "Step 2370, finished 1264 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.866666666666667\n",
      "\t\t\tmax episode duration: 58\n",
      "\t\tAvg Loss. Total: 0.6843392709891002; Actor: 0.5646380772193272; Critic: 0.4020202398300171\n",
      "Step 2400, finished 1282 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.333333333333332\n",
      "\t\t\tmax episode duration: 72\n",
      "\t\tAvg Loss. Total: 0.622247255841891; Actor: 0.5167558600505193; Critic: 0.3638693203528722\n",
      "Step 2430, finished 1299 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.533333333333335\n",
      "\t\t\tmax episode duration: 72\n",
      "\t\tAvg Loss. Total: 0.5989731252193451; Actor: 0.4943779597679774; Critic: 0.3517841468254725\n",
      "Step 2460, finished 1315 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.066666666666666\n",
      "\t\t\tmax episode duration: 58\n",
      "\t\tAvg Loss. Total: 0.6079309672117233; Actor: 0.48507494827111564; Critic: 0.3653934940695763\n",
      "Step 2490, finished 1328 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.433333333333334\n",
      "\t\t\tmax episode duration: 58\n",
      "\t\tAvg Loss. Total: 0.6937447031339009; Actor: 0.5483418931563695; Critic: 0.41957375009854636\n",
      "Step 2520, finished 1344 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.633333333333333\n",
      "\t\t\tmax episode duration: 59\n",
      "\t\tAvg Loss. Total: 0.6361388037602107; Actor: 0.5318142692248027; Critic: 0.37023167113463085\n",
      "Step 2550, finished 1361 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 25.533333333333335\n",
      "\t\t\tmax episode duration: 59\n",
      "\t\tAvg Loss. Total: 0.6791929215192795; Actor: 0.5509920199712117; Critic: 0.40369690557320914\n",
      "Step 2580, finished 1378 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.233333333333334\n",
      "\t\t\tmax episode duration: 44\n",
      "\t\tAvg Loss. Total: 0.5687605381011963; Actor: 0.470957288146019; Critic: 0.3332819034655889\n",
      "Step 2610, finished 1391 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.4\n",
      "\t\t\tmax episode duration: 48\n",
      "\t\tAvg Loss. Total: 0.6530950744946797; Actor: 0.5129612376292546; Critic: 0.3966144551833471\n",
      "Step 2640, finished 1412 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.533333333333335\n",
      "\t\t\tmax episode duration: 50\n",
      "\t\tAvg Loss. Total: 0.6212500999371211; Actor: 0.5152330656846364; Critic: 0.36363356908162436\n",
      "Step 2670, finished 1429 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.566666666666666\n",
      "\t\t\tmax episode duration: 50\n",
      "\t\tAvg Loss. Total: 0.5956733137369156; Actor: 0.4948366949955622; Critic: 0.3482549632589022\n",
      "Step 2700, finished 1449 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.2\n",
      "\t\t\tmax episode duration: 38\n",
      "\t\tAvg Loss. Total: 0.5527470946311951; Actor: 0.4517642199993134; Critic: 0.32686498363812766\n",
      "Step 2730, finished 1467 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 17.6\n",
      "\t\t\tmax episode duration: 38\n",
      "\t\tAvg Loss. Total: 0.5670205930868785; Actor: 0.4697493096192678; Critic: 0.3321459432442983\n",
      "Step 2760, finished 1485 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.166666666666668\n",
      "\t\t\tmax episode duration: 65\n",
      "\t\tAvg Loss. Total: 0.6956312239170075; Actor: 0.5492927352587382; Critic: 0.4209848493337631\n",
      "Step 2790, finished 1502 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.766666666666666\n",
      "\t\t\tmax episode duration: 65\n",
      "\t\tAvg Loss. Total: 0.599615541100502; Actor: 0.49048127929369606; Critic: 0.35437490344047545\n",
      "Step 2820, finished 1519 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.1\n",
      "\t\t\tmax episode duration: 39\n",
      "\t\tAvg Loss. Total: 0.6315499077240626; Actor: 0.5035938481489818; Critic: 0.37975298215945563\n",
      "Step 2850, finished 1533 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.933333333333334\n",
      "\t\t\tmax episode duration: 49\n",
      "\t\tAvg Loss. Total: 0.6687199095884959; Actor: 0.5426365623871485; Critic: 0.3974016269048055\n",
      "Step 2880, finished 1548 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.9\n",
      "\t\t\tmax episode duration: 55\n",
      "\t\tAvg Loss. Total: 0.6306762317816417; Actor: 0.5050749272108078; Critic: 0.3781387661894163\n",
      "Step 2910, finished 1566 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.333333333333332\n",
      "\t\t\tmax episode duration: 114\n",
      "\t\tAvg Loss. Total: 0.6156256934007008; Actor: 0.5098220070203145; Critic: 0.3607146938641866\n",
      "Step 2940, finished 1586 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.333333333333332\n",
      "\t\t\tmax episode duration: 114\n",
      "\t\tAvg Loss. Total: 0.5537382791439692; Actor: 0.4507077972094218; Critic: 0.3283843810359637\n",
      "Step 2970, finished 1603 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 17.366666666666667\n",
      "\t\t\tmax episode duration: 35\n",
      "\t\tAvg Loss. Total: 0.6377297590176264; Actor: 0.5183934440215429; Critic: 0.37853303949038186\n",
      "Step 3000, finished 1619 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.666666666666668\n",
      "\t\t\tmax episode duration: 83\n",
      "\t\tAvg Loss. Total: 0.697167573372523; Actor: 0.5620341906944911; Critic: 0.4161504805088043\n",
      "Step 3030, finished 1633 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.066666666666666\n",
      "\t\t\tmax episode duration: 83\n",
      "\t\tAvg Loss. Total: 0.6579226573308309; Actor: 0.539870481689771; Critic: 0.3879874179760615\n",
      "Step 3060, finished 1649 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.9\n",
      "\t\t\tmax episode duration: 62\n",
      "\t\tAvg Loss. Total: 0.6515476028124492; Actor: 0.5269823988278707; Critic: 0.3880564014116923\n",
      "Step 3090, finished 1667 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.9\n",
      "\t\t\tmax episode duration: 81\n",
      "\t\tAvg Loss. Total: 0.6489936470985412; Actor: 0.5308742056290309; Critic: 0.3835565368334452\n",
      "Step 3120, finished 1678 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.633333333333333\n",
      "\t\t\tmax episode duration: 81\n",
      "\t\tAvg Loss. Total: 0.6876426180203755; Actor: 0.5630950580040613; Critic: 0.40609509746233624\n",
      "Step 3150, finished 1696 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.9\n",
      "\t\t\tmax episode duration: 45\n",
      "\t\tAvg Loss. Total: 0.672258996963501; Actor: 0.5563036759694417; Critic: 0.3941071589787801\n",
      "Step 3180, finished 1714 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.066666666666666\n",
      "\t\t\tmax episode duration: 68\n",
      "\t\tAvg Loss. Total: 0.5833131531874339; Actor: 0.4769901305437088; Critic: 0.3448180903991063\n",
      "Step 3210, finished 1732 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.766666666666666\n",
      "\t\t\tmax episode duration: 48\n",
      "\t\tAvg Loss. Total: 0.5710417687892914; Actor: 0.46488814254601796; Critic: 0.3385976935426394\n",
      "Step 3240, finished 1751 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.533333333333335\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.5888851513465245; Actor: 0.48315958778063456; Critic: 0.34730536142985025\n",
      "Step 3270, finished 1769 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.8\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.6449970185756684; Actor: 0.5126040379206339; Critic: 0.38869499961535137\n",
      "Step 3300, finished 1787 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.133333333333333\n",
      "\t\t\tmax episode duration: 41\n",
      "\t\tAvg Loss. Total: 0.5523751010497411; Actor: 0.46386097073554994; Critic: 0.3204446196556091\n",
      "Step 3330, finished 1808 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 17.5\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.596557155251503; Actor: 0.4698762898643812; Critic: 0.3616190155347188\n",
      "Step 3360, finished 1825 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.1\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.5921951125065485; Actor: 0.4844171663125356; Critic: 0.34998652835687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3390, finished 1843 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.066666666666666\n",
      "\t\t\tmax episode duration: 50\n",
      "\t\tAvg Loss. Total: 0.5853992899258932; Actor: 0.4732522785663605; Critic: 0.3487731501460075\n",
      "Step 3420, finished 1859 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.4\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.6061707218488057; Actor: 0.49093963702519733; Critic: 0.3607009038329124\n",
      "Step 3450, finished 1879 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.3\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.5441399524609248; Actor: 0.4538811335961024; Critic: 0.31719938417275745\n",
      "Step 3480, finished 1895 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.8\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.5961657454570134; Actor: 0.47072158257166546; Critic: 0.3608049601316452\n",
      "Step 3510, finished 1913 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.533333333333335\n",
      "\t\t\tmax episode duration: 56\n",
      "\t\tAvg Loss. Total: 0.6286438425381978; Actor: 0.5008423735698064; Critic: 0.37822265625\n",
      "Step 3540, finished 1931 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.666666666666668\n",
      "\t\t\tmax episode duration: 51\n",
      "\t\tAvg Loss. Total: 0.6236640294392903; Actor: 0.4985471576452255; Critic: 0.37439044515291847\n",
      "Step 3570, finished 1949 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.733333333333334\n",
      "\t\t\tmax episode duration: 51\n",
      "\t\tAvg Loss. Total: 0.6285457829634349; Actor: 0.5141509095827739; Critic: 0.37147032767534255\n",
      "Step 3600, finished 1965 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.0\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.5992649286985398; Actor: 0.4646305988232295; Critic: 0.3669496297836304\n",
      "Step 3630, finished 1983 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.3\n",
      "\t\t\tmax episode duration: 46\n",
      "\t\tAvg Loss. Total: 0.5887688348690668; Actor: 0.4810728758573532; Critic: 0.34823239892721175\n",
      "Step 3660, finished 2001 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.666666666666668\n",
      "\t\t\tmax episode duration: 39\n",
      "\t\tAvg Loss. Total: 0.5188836365938186; Actor: 0.43245454827944435; Critic: 0.3026563649376233\n",
      "Step 3690, finished 2021 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 17.6\n",
      "\t\t\tmax episode duration: 39\n",
      "\t\tAvg Loss. Total: 0.5324039856592814; Actor: 0.4318155348300934; Critic: 0.31649622271458305\n",
      "Step 3720, finished 2041 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.966666666666665\n",
      "\t\t\tmax episode duration: 42\n",
      "\t\tAvg Loss. Total: 0.5732876151800156; Actor: 0.46159660120805107; Critic: 0.34248931109905245\n",
      "Step 3750, finished 2059 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.5\n",
      "\t\t\tmax episode duration: 58\n",
      "\t\tAvg Loss. Total: 0.5412956655025483; Actor: 0.4266216278076172; Critic: 0.3279848575592041\n",
      "Step 3780, finished 2079 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 17.833333333333332\n",
      "\t\t\tmax episode duration: 47\n",
      "\t\tAvg Loss. Total: 0.4765434046586355; Actor: 0.3873836904764175; Critic: 0.2828515633940697\n",
      "Step 3810, finished 2096 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 18.966666666666665\n",
      "\t\t\tmax episode duration: 47\n",
      "\t\tAvg Loss. Total: 0.5495745559533437; Actor: 0.43932757278283435; Critic: 0.3299107705553373\n",
      "Step 3840, finished 2114 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.533333333333335\n",
      "\t\t\tmax episode duration: 35\n",
      "\t\tAvg Loss. Total: 0.5370797137419383; Actor: 0.45138283471266427; Critic: 0.31138829489549\n",
      "Step 3870, finished 2127 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.5\n",
      "\t\t\tmax episode duration: 61\n",
      "\t\tAvg Loss. Total: 0.6445369005203248; Actor: 0.5117370883623759; Critic: 0.38866835782925285\n",
      "Step 3900, finished 2143 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.7\n",
      "\t\t\tmax episode duration: 61\n",
      "\t\tAvg Loss. Total: 0.5626119077205658; Actor: 0.45071401993433635; Critic: 0.33725490073362985\n",
      "Step 3930, finished 2158 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 21.666666666666668\n",
      "\t\t\tmax episode duration: 57\n",
      "\t\tAvg Loss. Total: 0.6091936886310577; Actor: 0.4950361490249634; Critic: 0.36167561411857607\n",
      "Step 3960, finished 2173 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.166666666666668\n",
      "\t\t\tmax episode duration: 57\n",
      "\t\tAvg Loss. Total: 0.640260128180186; Actor: 0.5187885542710622; Critic: 0.3808658589919408\n",
      "Step 3990, finished 2190 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 20.733333333333334\n",
      "\t\t\tmax episode duration: 38\n",
      "\t\tAvg Loss. Total: 0.5707289338111877; Actor: 0.46129927933216097; Critic: 0.3400792966286341\n",
      "Step 4020, finished 2207 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.3\n",
      "\t\t\tmax episode duration: 93\n",
      "\t\tAvg Loss. Total: 0.6440907518068949; Actor: 0.5104016979535421; Critic: 0.3888898968696594\n",
      "Step 4050, finished 2223 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 23.466666666666665\n",
      "\t\t\tmax episode duration: 93\n",
      "\t\tAvg Loss. Total: 0.614007298151652; Actor: 0.5178144683440526; Critic: 0.3551000644763311\n",
      "Step 4080, finished 2232 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 22.033333333333335\n",
      "\t\t\tmax episode duration: 70\n",
      "\t\tAvg Loss. Total: 0.6774851481119791; Actor: 0.5320694675048192; Critic: 0.41145041783650715\n",
      "Step 4110, finished 2254 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 24.533333333333335\n",
      "\t\t\tmax episode duration: 70\n",
      "\t\tAvg Loss. Total: 0.5912429600954056; Actor: 0.4726949801047643; Critic: 0.3548954705397288\n",
      "Step 4140, finished 2270 / 10000 episodes. Last 30 episodes:\n",
      "\t\t\tavg episode duration: 19.6\n",
      "\t\t\tmax episode duration: 67\n",
      "\t\tAvg Loss. Total: 0.5991064002116521; Actor: 0.49942339956760406; Critic: 0.34939470489819846\n"
     ]
    }
   ],
   "source": [
    "class QueueReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self) >= self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self):\n",
    "            return None\n",
    "        \n",
    "        s, a, r, t, d = zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "#         print(len(s), len(a), len(r), len(t), len(d))\n",
    "        \n",
    "        s = torch.cat(s)\n",
    "        t = torch.cat(t)\n",
    "        \n",
    "        r = torch.tensor(r, dtype=torch.float)\n",
    "        d = torch.tensor(d, dtype=torch.float)\n",
    "        a = torch.stack(a)\n",
    "        \n",
    "#         print(s.shape, a.shape, r.shape, t.shape, d.shape)\n",
    "        \n",
    "#         assert s.shape == (batch_size, 3, 40, 80)\n",
    "#         assert t.shape == (batch_size, 3, 40, 80)\n",
    "#         assert r.shape == (batch_size,)\n",
    "#         assert d.shape == (batch_size,)\n",
    "#         assert a.shape == (batch_size,)\n",
    "        \n",
    "        return s, a, r, t, d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "def select_action(model, state):\n",
    "    log_p = model(state)\n",
    "    p = torch.exp(log_p)\n",
    "#     print(p.mean(0), p.var(dim=0))\n",
    "    action = torch.multinomial(p, 1)\n",
    "#     print(p[:, 0].data.view(-1), action.view(-1))\n",
    "    log_p = log_p.gather(1, action).view(-1)\n",
    "    action = action.view(-1)\n",
    "#     n = len(state)\n",
    "#     assert action.size() == (n, )\n",
    "#     assert log_p.size() == (n, )\n",
    "    return action, log_p\n",
    "\n",
    "def train_actor_critic(actor, critic, memory, optimizer, log_ps, state, reward, next_state, done, discount_factor):\n",
    "    \n",
    "    batch = memory.sample(32)\n",
    "    \n",
    "    if batch is None or len(memory) < 128:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    state, action, reward, next_state, done = batch\n",
    "    \n",
    "    value = critic(state)\n",
    "    \n",
    "    # target = R + gamma * V(S')\n",
    "    dones = done.type(torch.FloatTensor).reshape(-1)\n",
    "    reward = reward.reshape(-1)\n",
    "    next_values = critic(next_state)\n",
    "    target = reward + discount_factor * next_values * (1 - dones)\n",
    "    \n",
    "    adv = target - value\n",
    "    value_loss = F.smooth_l1_loss(adv, torch.zeros_like(adv))\n",
    "    \n",
    "    log_ps = actor(state).gather(1, action.view(-1, 1)).view(-1)\n",
    "    actor_loss = -1 * (adv.detach() * log_ps).mean()\n",
    "    \n",
    "    # The loss is composed of the value_loss (for the critic) and the actor_loss\n",
    "    loss = value_loss + actor_loss * 0.5\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "#     print(\n",
    "#         sum([torch.sum(x.grad.data**2).item() for x in actor.parameters()]),\n",
    "#         sum([torch.sum(x.grad.data**2).item() for x in critic.parameters()])\n",
    "#     )\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), value_loss.item(), actor_loss.item()  # Returns a Python scalar, and releases history (similar to .detach())\n",
    "\n",
    "def run_episodes_actor_critic(actor, critic, envs, max_episodes, max_steps, discount_factor, actor_learn_rate, critic_learn_rate):\n",
    "    \n",
    "    # We can use a single optimizer for both the actor and the critic, even with separate learn rates\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': actor.parameters(), 'lr': actor_learn_rate},\n",
    "        {'params': critic.parameters(), 'lr': critic_learn_rate}\n",
    "    ])\n",
    "    \n",
    "    episode_durations = []\n",
    "    state = torch.cat([env.reset() for env in envs])\n",
    "    current_episode_lengths = torch.zeros(len(envs), dtype=torch.int64)\n",
    "    step_losses = []  # Keep track of losses for plotting\n",
    "    \n",
    "    memory = QueueReplayMemory(200)\n",
    "    \n",
    "    LOG = 30\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "\n",
    "        if i % LOG == 0 and i > 0:\n",
    "            print(f\"Step {i}, finished {len(episode_durations)} / {max_episodes} episodes. Last {LOG} episodes:\")\n",
    "            print(f\"\\t\\t\\tavg episode duration: {np.mean(episode_durations[-LOG:])}\")\n",
    "            print(f\"\\t\\t\\tmax episode duration: {max(episode_durations[-LOG:])}\")\n",
    "\n",
    "        action, log_ps = select_action(actor, state)\n",
    "        next_state, reward, done, _ = zip(*[env.step(a.item()) for env, a in zip(envs, action)])\n",
    "        \n",
    "        for t in zip(state.split(1, dim=0), action, reward, next_state, done):\n",
    "            memory.push(t)\n",
    "        \n",
    "        next_state = torch.cat(next_state)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "        current_episode_lengths += 1\n",
    "\n",
    "        losses = train_actor_critic(actor, critic, memory, optimizer, log_ps, state, reward, next_state, done, discount_factor)\n",
    "\n",
    "        step_losses.append(losses)\n",
    "\n",
    "        if i % LOG == 0 and i > 0:\n",
    "            last_losses = step_losses[-LOG:]\n",
    "            t_loss = np.mean([l for l, _, _  in last_losses])\n",
    "            c_loss = np.mean([c for _, c, _  in last_losses])\n",
    "            a_loss = np.mean([a for _, _, a  in last_losses])\n",
    "            print(f\"\\t\\tAvg Loss. Total: {t_loss}; Actor: {a_loss}; Critic: {c_loss}\")\n",
    "\n",
    "        episode_durations.extend(current_episode_lengths[done])\n",
    "        current_episode_lengths[done] = 0  # PyTorch can also work in place\n",
    "        \n",
    "        # Reset envs that are done\n",
    "        next_state = [\n",
    "            env.reset() if d else s\n",
    "            for env, s, d in zip(envs, next_state.split(1, dim=0), done)\n",
    "        ]\n",
    "        next_state = torch.cat(next_state)\n",
    "        state = next_state\n",
    "\n",
    "        # Check if we have finished sufficiently many episodes\n",
    "        if len(episode_durations) >= max_episodes:\n",
    "            break\n",
    "    \n",
    "    return episode_durations[:max_episodes], step_losses  # In case we want exactly num_episodes returned\n",
    "\n",
    "\n",
    "# Maybe you should make it a bit deeper?\n",
    "class DeepPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.cl1 = nn.Conv2d(3, 32, 5, padding=0, stride=2)\n",
    "#         self.pl1 = nn.MaxPool2d(2)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.cl2 = nn.Conv2d(32, 128, 5, padding=0, stride=2)\n",
    "#         self.pl2 = nn.MaxPool2d(2)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "#         self.cl3 = nn.Conv2d(64, 128, 5, padding=0, stride=2)\n",
    "#         self.pl3 = nn.MaxPool2d(2)\n",
    "#         self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.ap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.l1 = nn.Linear(5*10*32, 2)\n",
    "        self.l1 = nn.Linear(128, 32)\n",
    "        self.l2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        x = self.cl1(x)\n",
    "#         x = self.pl1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.cl2(x)\n",
    "#         x = self.pl2(x)\n",
    "#         x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "#         x = self.cl3(x)\n",
    "#         x = self.pl3(x)\n",
    "#         x = self.bn3(x)\n",
    "#         x = torch.relu(x)\n",
    "        \n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.l1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l2(x)\n",
    "    \n",
    "        return F.log_softmax(x, -1)\n",
    "    \n",
    "class DeepCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.cl1 = nn.Conv2d(3, 64, 5, padding=0, stride=2)\n",
    "#         self.pl1 = nn.MaxPool2d(2)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.cl2 = nn.Conv2d(64, 128, 5, padding=0, stride=2)\n",
    "#         self.pl2 = nn.MaxPool2d(2)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.ap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.l1 = nn.Linear(128, 32)\n",
    "#         self.l1 = nn.Linear(7 * 17 * 64, 1)\n",
    "        self.l2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        x = self.cl1(x)\n",
    "#         x = self.pl1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.cl2(x)\n",
    "#         x = self.pl2(x)\n",
    "#         x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.l1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.l2(x).view(-1)\n",
    "\n",
    "#         x = self.l1(x).view(-1)\n",
    "    \n",
    "        return x\n",
    "\n",
    "def plot_trends(episode_durations, step_losses):\n",
    "    plt.plot(smooth(episode_durations, 20))\n",
    "    plt.title('Episode durations')\n",
    "    plt.show()\n",
    "    loss, v_loss, a_loss = zip(*step_losses)\n",
    "\n",
    "    plt.plot(smooth(v_loss, 100))\n",
    "    plt.title('Value loss')\n",
    "    plt.show()\n",
    "    plt.plot(smooth(a_loss, 100))\n",
    "    plt.title('Actor loss')\n",
    "    plt.show()\n",
    "    \n",
    "policy = DeepPolicy()\n",
    "critic = DeepCritic()\n",
    "filename = 'weights.pt'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    print(f\"Loading weights from {filename}\")\n",
    "    weights = torch.load(filename)\n",
    "    \n",
    "    policy.load_state_dict(weights['policy'])\n",
    "    critic.load_state_dict(weights['critic'])\n",
    "    \n",
    "else:\n",
    "    # Train\n",
    "    \n",
    "    ### TODO some training here, maybe? Or run this on a different machine?    \n",
    "    num_envs = 12\n",
    "    max_steps = 10000\n",
    "    max_episodes = 10000\n",
    "    discount_factor = 0.99\n",
    "    lr_actor = 1e-4\n",
    "    lr_critic = 1e-3\n",
    "    seed = 42\n",
    "\n",
    "    envs = [CartPoleRawEnv() for i in range(num_envs)]\n",
    "\n",
    "    for i, env in enumerate(envs):\n",
    "        env.seed(seed + i)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    try:\n",
    "        episode_durations, step_losses = run_episodes_actor_critic(policy, critic, envs, max_episodes, max_steps, discount_factor, lr_actor, lr_critic)\n",
    "    except KeyboardInterrupt as err:\n",
    "        for e in envs:\n",
    "            e.close()\n",
    "        raise err\n",
    "    \n",
    "    plot_trends(episode_durations, step_losses)\n",
    "        \n",
    "    print(f\"Saving weights to {filename}\")\n",
    "    torch.save({\n",
    "        # You can add more here if you need, e.g. critic\n",
    "        'policy': policy.state_dict(),  # Always save weights rather than objects\n",
    "        'critic': critic.state_dict(),  # Always save weights rather than objects\n",
    "    },\n",
    "    filename)\n",
    "    \n",
    "    plot_trends(episode_durations, step_losses)\n",
    "    \n",
    "def bonus_get_action(x):\n",
    "    return policy(x).exp().multinomial(1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in envs:\n",
    "    e.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "episode_durations = []\n",
    "for i in range(20):  # Not too many since it may take forever to render\n",
    "    test_env = CartPoleRawEnv()\n",
    "    test_env.seed(seed + i)\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        with torch.no_grad():\n",
    "            action = bonus_get_action(state).item()\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "    episode_durations.append(steps)\n",
    "    test_env.close()\n",
    "    \n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b800bfb91f987f14e0c35bc0c41d538b",
     "grade": true,
     "grade_id": "cell-0d7bd58a23fdfabb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "episode_durations = []\n",
    "for i in range(20):  # Not too many since it may take forever to render\n",
    "    test_env = CartPoleRawEnv()\n",
    "    test_env.seed(seed + i)\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        with torch.no_grad():\n",
    "            action = bonus_get_action(state).item()\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "    episode_durations.append(steps)\n",
    "    test_env.close()\n",
    "    \n",
    "plt.plot(smooth(episode_durations, 100))\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
